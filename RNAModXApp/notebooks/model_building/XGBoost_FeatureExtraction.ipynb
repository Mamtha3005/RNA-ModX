{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import All Libraries Here\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb \n",
    "import statistics\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from igraph import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Constants\n",
    "INPUT_TRAIN_IN = \"../../data/train_in.csv\"\n",
    "INPUT_TRAIN_OUT = \"../../data/train_out.csv\"  \n",
    "INPUT_TEST_IN = \"../../data/test_in.csv\"\n",
    "INPUT_TEST_OUT = \"../../data/test_out.csv\"\n",
    "\n",
    "TARGET_MODEL_PATH = '../../webapp/model_files'\n",
    "\n",
    "WINDOW_SIZE =  50 # Final RNA Sequence will be 101 Length \n",
    "MAX_LENGTH = (WINDOW_SIZE*2) +1\n",
    "ENCODING_METHOD = 4 # 1 - ANF Encoding , 2 - One Hot Encoding  , 3 - Complex Network  , 4 - Word2Vec Model\n",
    "PERFORM_DATA_BALANCING = True\n",
    "K_MERS_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read X Varaibles and Y Varaibles\n",
    "\n",
    "x_train_raw =  pd.read_csv(INPUT_TRAIN_IN, header=None , skiprows=1 )\n",
    "y_train_raw =  pd.read_csv(INPUT_TRAIN_OUT, header=None , skiprows=1 )\n",
    "\n",
    "x_test_raw =  pd.read_csv(INPUT_TEST_IN, header=None , skiprows=1)\n",
    "y_test_raw =  pd.read_csv(INPUT_TEST_OUT, header=None , skiprows=1)\n",
    "\n",
    "print(\"Shape of Input Feature - \" , x_train_raw.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sequence Positions to extracted from Original Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_index = (x_train_raw.shape[1] // 2) + 1\n",
    "STRAT_INEDX =middle_index - WINDOW_SIZE -1 \n",
    "END_INDEX =middle_index + WINDOW_SIZE "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Graph - Complex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_graph(thresholdCN):\n",
    "    metrics = []\n",
    "    metrics.append(mean(thresholdCN.betweenness(directed=False, weights=None)))\n",
    "    metrics.append(mean(thresholdCN.degree()))\n",
    "    metrics.append(thresholdCN.assortativity_degree(directed=False))  # Returns the assortativity\n",
    "    metrics.append(max(thresholdCN.degree()))\n",
    "    metrics.append(min(thresholdCN.degree()))\n",
    "    metrics.append(np.std(thresholdCN.degree()))  # Returns the strength (weighted degree)\n",
    "    metrics.append(thresholdCN.average_path_length(directed=False, unconn=False))  # Average path length\n",
    "    metrics.append(thresholdCN.transitivity_avglocal_undirected())  # local transitivity (clustering coefficient)\n",
    "    metrics.append(thresholdCN.transitivity_undirected())  # global transitivity (clustering coefficient)\n",
    "    metrics.append(cn.ecount())  # Counts the number of edges\n",
    "    metrics.append(thresholdCN.motifs_randesu_no(size=3))\n",
    "    metrics.append(thresholdCN.motifs_randesu_no(size=4))\n",
    "    metrics.append(mean(thresholdCN.authority_score()))\n",
    "    metrics.append(mean(thresholdCN.closeness(vertices=None, mode=ALL, cutoff=None, weights=None, normalized=True)))  # Calculates the closeness centralities of given vertices in a graph\n",
    "    metrics.append(mean(thresholdCN.constraint(vertices=None, weights=None)))  # Calculates Burt's constraint scores for given vertices in a graph.\n",
    "    metrics.append(mean(thresholdCN.count_multiple(edges=None)))  # Counts the multiplicities of the given edges.\n",
    "    metrics.append(thresholdCN.density(loops=False))  # Calculates the density of the graph.\n",
    "    metrics.append(thresholdCN.diameter(directed=False, unconn=False, weights=None))  # Calculates the diameter of the graph.\n",
    "    metrics.append(mean(thresholdCN.eccentricity(vertices=None, mode=ALL)))  # Calculates the eccentricities of given vertices in a graph.\n",
    "    metrics.append(mean(thresholdCN.edge_betweenness(directed=False, cutoff=None, weights=None)))  # Calculates or estimates the edge betweennesses in a graph.\n",
    "    metrics.append(mean(thresholdCN.hub_score()))  # Calculates Kleinberg's hub score for the vertices of the graph.\n",
    "    metrics.append(thresholdCN.maxdegree())  # Returns the maximum degree of a vertex set in the graph.\n",
    "    metrics.append(mean(thresholdCN.neighborhood_size()))  # For each vertex specified by vertices, returns the number of vertices reachable from that vertex in at most order steps\n",
    "    metrics.append(thresholdCN.radius())  # Calculates the radius of the graph.\n",
    "    metrics.append(mean(thresholdCN.strength()))  # Returns the strength (weighted degree) of some vertices from the graph.\n",
    "    metrics.append(cn.vcount())  # Counts the number of vertices.\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def patterns(seq, win):\n",
    "    \"\"\"\n",
    "    Generate k-mers: subsequences of length k\n",
    "    contained in a biological sequence.\n",
    "    \"\"\"\n",
    "    seqlen = len(seq)\n",
    "    for i in range(seqlen):\n",
    "        j = seqlen if i + win > seqlen else i + win\n",
    "        yield seq[i:j]\n",
    "        if j == seqlen: break\n",
    "    return\n",
    "\n",
    "def complex_network(seq):\n",
    "    \"\"\"Generates complex network\"\"\"\n",
    "    global name_seq, cn\n",
    "    ksize = K_MERS_SIZE\n",
    "    threshold = 3\n",
    "\n",
    "    metrics = []\n",
    "    cn = Graph()\n",
    "    seq = seq.upper()\n",
    "    \n",
    "    for k in range(1, ksize + 1):\n",
    "        cn = Graph()\n",
    "        kmer = []\n",
    "        for subseq in patterns(seq, k):  # Generates k pattern\n",
    "            kmer.append(str(subseq))\n",
    "        #print(kmer)\n",
    "        vertices = np.unique(kmer)\n",
    "        for vert in vertices:\n",
    "            cn.add_vertices(vert)\n",
    "        for i in range(len(kmer)-1):  # Position -1 -- Build the Network\n",
    "            cn.add_edges([(kmer[i], kmer[i+1])])\n",
    "        # print(summary(cn))\n",
    "        metrics +=feature_extraction_graph(cn)\n",
    "        #print('Result Matric' , metrics)\n",
    "        #print('Size of Matric with k ' , k , '  Size ' , len(metrics))\n",
    "    metrics_preprocessing = np.nan_to_num(metrics)     \n",
    "    return metrics_preprocessing\n",
    "\n",
    "def encode_using_graph(x_train_raw):\n",
    "    truncated_df = x_train_raw.iloc[:,STRAT_INEDX :END_INDEX]\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    result = truncated_df['Sequence'].apply(complex_network)\n",
    "    return result \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply ANF Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ANF Encoding on Input Features\n",
    "def apply_accumulated_nucle_frequency(seq):\n",
    "    mapping = []\n",
    "    A = 0\n",
    "    C = 0\n",
    "    T = 0\n",
    "    G = 0\n",
    "    for i, v in seq.items():\n",
    "        if v == 'A':\n",
    "            A += 1\n",
    "            mapping.append(A / (i + 1))\n",
    "        elif v == 'C':\n",
    "            C += 1\n",
    "            mapping.append(C / (i + 1))\n",
    "        elif v == 'T' or v == 'U':\n",
    "            T += 1\n",
    "            mapping.append(T / (i + 1))\n",
    "        else:\n",
    "            G += 1\n",
    "            mapping.append(G / (i + 1))\n",
    "    padding = (MAX_LENGTH - len(mapping))\n",
    "    mapping = np.pad(mapping, (0, padding), 'constant')\n",
    "    return mapping\n",
    "\n",
    "# # Encode X Features Based on Middle Index\n",
    "def encode_using_anf(x_train_raw):\n",
    "    truncated_df = x_train_raw.iloc[:,STRAT_INEDX :END_INDEX]\n",
    "    print(truncated_df.shape)\n",
    "    result = truncated_df.apply(apply_accumulated_nucle_frequency, axis=1)\n",
    "    return result "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One Hot Encoding for RNA Sequence \n",
    "def one_hot_encode_sequence(seq):\n",
    "    bases = 'ACGT'\n",
    "    base_dict = {base: i for i, base in enumerate(bases)}\n",
    "    one_hot = np.zeros((len(seq), len(bases)))\n",
    "    for i, base in enumerate(seq):\n",
    "        if base == 'N':\n",
    "            continue\n",
    "        one_hot[i, base_dict[base]] = 1\n",
    "    return one_hot.flatten()\n",
    "\n",
    "\n",
    "# Encode X Features Based on Middle Index\n",
    "def encode_x_with_in_hot(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    print(truncated_df)\n",
    "    result =  truncated_df['Sequence'].apply(one_hot_encode_sequence)\n",
    "    return result \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Documentation - \n",
    "# Encode RNA sequences using Word2Vec embeddings\n",
    "def encode_sequence(sequence , word2vec_model):\n",
    "    embeddings = [word2vec_model.wv[kmer] for kmer in sequence]\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def applyKmersAndEncoding(seq):\n",
    "    k = 3\n",
    "    tokenized_sequences = [''.join(seq[i:i+k]) for i in range(0, len(seq)-k+1)]\n",
    "    return tokenized_sequences\n",
    "    \n",
    "\n",
    "\n",
    "def encode_x_with_word2Vec(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    tokenized_sequences =  truncated_df['Sequence'].apply(applyKmersAndEncoding).tolist()\n",
    "    \n",
    "    # The result, tokenized_sequences, is a list of lists, where each inner list\n",
    "    #  contains the k-mers of the corresponding RNA sequence from the truncated_df list. \n",
    "    embedding_size = 100\n",
    "    word2vec_model = gensim.models.Word2Vec(tokenized_sequences, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "    result = []\n",
    "    for seq in tokenized_sequences:\n",
    "        embedding = encode_sequence(seq , word2vec_model)\n",
    "        result.append(embedding)\n",
    "    return result \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Encoding of X Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Encoding Method\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "if ENCODING_METHOD == 1:\n",
    "    x_encoded = encode_using_anf(x_train_raw).tolist()\n",
    "\n",
    "if ENCODING_METHOD == 2:\n",
    "    x_train = pd.DataFrame()\n",
    "    x_train['one_hot_encoded'] = encode_x_with_in_hot(x_train_raw)\n",
    "    x_encoded = np.array(x_train['one_hot_encoded'].tolist())\n",
    "\n",
    "if ENCODING_METHOD == 3:\n",
    "    x_encoded = encode_using_graph(x_train_raw).tolist()\n",
    "\n",
    "if ENCODING_METHOD == 4:\n",
    "     x_encoded = encode_x_with_word2Vec(x_train_raw)\n",
    "     x_test_encoded = encode_x_with_word2Vec(x_test_raw)\n",
    "\n",
    "     print(x_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist Feature to a File.\n",
    "import csv\n",
    "with open('Complext_Network_Feature.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile , delimiter = ',')\n",
    "    for row in x_encoded:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Each Y category to 0 to 12 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode Y to its Original Form . Additional Class - NonMoD added for Non Modified RNA Sequence \n",
    "RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
    "RMEncoding = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "def decode_y(row):   \n",
    "    decoded = ''\n",
    "    for index , n in enumerate(row.tolist()) :\n",
    "        if n == 1 :\n",
    "            decoded = RMEncoding[index]\n",
    "    \n",
    "    if decoded == '':\n",
    "        return 12\n",
    "    \n",
    "    return decoded \n",
    "\n",
    "y_encoded =  y_train_raw.apply(decode_y , axis=1)\n",
    "y_test_encoded = y_test_raw.apply(decode_y,axis =1)\n",
    "\n",
    "y_encoded.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Sample Data Using RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Reference : https://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/train.py\n",
    "\n",
    "# PERFORM_DATA_BALANCING - if True then code will perform over sampling for each category.\n",
    "if PERFORM_DATA_BALANCING:\n",
    "    sm = RandomOverSampler(random_state=42)\n",
    "    x_encoded, y_encoded = sm.fit_resample(x_encoded, y_encoded)\n",
    "    print(y_encoded.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train anf Test Data for Model training - 80:20 Split \n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = x_encoded\n",
    "y_train = y_encoded\n",
    "X_test = x_test_encoded\n",
    "y_test = y_test_encoded\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 13,\n",
    "    'max_depth': 4,\n",
    "    'n_jobs': -1,\n",
    "    'enable_categorical': True\n",
    "}\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Train XGBoost model\n",
    "num_rounds = 100    \n",
    "model = xgb.train(params, dtrain, num_rounds) \n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(dtest) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix \n",
    "cm = confusion_matrix(y_test, y_pred, labels=np.array(RMEncoding))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=np.array(RMEncoding))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)\n",
    "\n",
    "print(\"Accuracy of XGBoost  model: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# TARGET_MODEL_PATH += '/xgboost_model.bin'\n",
    "\n",
    "# ##dump the model into a file\n",
    "# with open(TARGET_MODEL_PATH, 'wb') as f_out:\n",
    "#     pickle.dump(bst, f_out) # write final_model in .bin file\n",
    "#     f_out.close()  # close the file \n",
    "#     print(\"Exported Model Successfully\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Model Excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ---------------------- Further Feature Extraction with ANF Fourier Transform -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_extraction(features, spectrum, spectrumTwo):\n",
    "#     average = sum(spectrum) / len(spectrum)\n",
    "#     features.append(average)\n",
    "#     ###################################\n",
    "#     median = np.median(spectrum)\n",
    "#     features.append(median)\n",
    "#     ###################################\n",
    "#     maximum = np.max(spectrum)\n",
    "#     features.append(maximum)\n",
    "#     ###################################\n",
    "#     minimum = np.min(spectrum)\n",
    "#     features.append(minimum)\n",
    "#     ###################################\n",
    "#     peak = (len(spectrum) / 3) / (average)\n",
    "#     features.append(peak)\n",
    "#     ###################################\n",
    "#     peak_two = (len(spectrumTwo) / 3) / (np.mean(spectrumTwo))\n",
    "#     features.append(peak_two)\n",
    "#     ###################################\n",
    "#     standard_deviation = np.std(spectrum)  # standard deviation\n",
    "#     features.append(standard_deviation)\n",
    "#     ###################################\n",
    "#     standard_deviation_pop = statistics.stdev(spectrum)  # population sample standard deviation\n",
    "#     features.append(standard_deviation_pop)\n",
    "#     ###################################\n",
    "#     percentile15 = np.percentile(spectrum, 15)\n",
    "#     features.append(percentile15)\n",
    "#     ###################################\n",
    "#     percentile25 = np.percentile(spectrum, 25)\n",
    "#     features.append(percentile25)\n",
    "#     ###################################\n",
    "#     percentile50 = np.percentile(spectrum, 50)\n",
    "#     features.append(percentile50)\n",
    "#     ###################################\n",
    "#     percentile75 = np.percentile(spectrum, 75)\n",
    "#     features.append(percentile75)\n",
    "#     ###################################\n",
    "#     amplitude = maximum - minimum\n",
    "#     features.append(amplitude)\n",
    "#     ###################################\n",
    "#     # mode = statistics.mode(spectrum)\n",
    "#     ###################################\n",
    "#     variance = statistics.variance(spectrum)\n",
    "#     features.append(variance)\n",
    "#     ###################################\n",
    "#     interquartile_range = np.percentile(spectrum, 75) - np.percentile(spectrum, 25)\n",
    "#     features.append(interquartile_range)\n",
    "#     ###################################\n",
    "#     semi_interquartile_range = (np.percentile(spectrum, 75) - np.percentile(spectrum, 25)) / 2\n",
    "#     features.append(semi_interquartile_range)\n",
    "#     ###################################\n",
    "#     coefficient_of_variation = standard_deviation / average\n",
    "#     features.append(coefficient_of_variation)\n",
    "#     ###################################\n",
    "#     skewness = (3 * (average - median)) / standard_deviation\n",
    "#     features.append(skewness)\n",
    "#     ###################################\n",
    "#     kurtosis = (np.percentile(spectrum, 75) - np.percentile(spectrum, 25)) / (\n",
    "#             2 * (np.percentile(spectrum, 90) - np.percentile(spectrum, 10)))\n",
    "#     features.append(kurtosis)\n",
    "#     ###################################\n",
    "#     return\n",
    "\n",
    "# def accumulated_nucle_frequency_fourier(seq):\n",
    "#     mapping = []\n",
    "#     spectrumTwo = []\n",
    "#     spectrum = []\n",
    "#     features = []\n",
    "#     A = 0\n",
    "#     C = 0\n",
    "#     T = 0\n",
    "#     G = 0\n",
    "    \n",
    "#     for i , v in seq.items():\n",
    "#         if v == 'A':\n",
    "#             A += 1\n",
    "#             mapping.append(A / (i + 1))\n",
    "#         elif v == 'C':\n",
    "#             C += 1\n",
    "#             mapping.append(C / (i + 1))\n",
    "#         elif v == 'T' or seq[i] == 'U':\n",
    "#             T += 1\n",
    "#             mapping.append(T / (i + 1))\n",
    "#         else:\n",
    "#             G += 1\n",
    "#             mapping.append(G / (i + 1))\n",
    "            \n",
    "#     Fmap = fft(mapping)\n",
    "#     for i in range(len(mapping)):\n",
    "#         specTotal = (abs(Fmap[i]) ** 2)\n",
    "#         specTwo = (abs(Fmap[i]))\n",
    "#         spectrum.append(specTotal)\n",
    "#         spectrumTwo.append(specTwo)\n",
    "#     feature_extraction(features, spectrum, spectrumTwo)\n",
    "#     return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode with Fourier Transform \n",
    "# def encode_x_with_fourier(x_train_raw):\n",
    "#     middle_index = (x_train_raw.shape[1] // 2) + 1\n",
    "#     truncated_df = x_train_raw.iloc[:, middle_index - WINDOW_SIZE - 1: middle_index + WINDOW_SIZE]\n",
    "#     result = truncated_df.apply(accumulated_nucle_frequency_fourier, axis=1)\n",
    "#     return result \n",
    "\n",
    "# x_train_encoded_fourier = encode_x_with_fourier(x_train_raw)\n",
    "\n",
    "# print(x_train_encoded_fourier.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Model Again \n",
    "\n",
    "# x_train_f= x_train_encoded_fourier.tolist()\n",
    "# y_train_f = y_train_original \n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=42)\n",
    "# x_res_f, y_res_f = sm.fit_resample(x_train_f, y_train_f)\n",
    "\n",
    "# #print(\"Shape of X Train : \" , x_train.shape , \" Shape of Y Train : \" , y_train.shape)\n",
    "\n",
    "# xg_train_f = xgb.DMatrix(x_res_f, label=y_res_f)\n",
    "\n",
    "# # setup parameters for xgboost\n",
    "# param = {}\n",
    "# # use softmax multi-class classification\n",
    "# param['objective'] = 'multi:softmax'\n",
    "# # scale weight of positive examples\n",
    "# param['eta'] = 0.1\n",
    "# param['max_depth'] = 6\n",
    "# param['nthread'] = 4\n",
    "# param['num_class'] = 13\n",
    "\n",
    "\n",
    "# num_round = 5\n",
    "# xgb_f = xgb.train(param, xg_train_f, num_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict Accuracy\n",
    "# y_test=  y_test_raw.apply(decode_y , axis=1)\n",
    "\n",
    "# x_test = encode_x_with_fourier(x_test_raw).tolist()\n",
    "# xg_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "# y_pred = xgb_f.predict(xg_test)\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "# # Calculate the accuracy score of the xgboost regression model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy of XGBoost  model: {:.2f}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------- Use One Hot Encoding ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode using one-hot encoding \n",
    "# def apply_one_hot_encode(sequence):\n",
    "#     nucleotides = ['C', 'A', 'T', 'G']\n",
    "#     one_hot = []\n",
    "#     for nucleotide in sequence:\n",
    "#         # For G - [0,0,1,0]\n",
    "#         if nucleotide == 'N': # N is not application , used for empty values.\n",
    "#             hot = [0,0,0,0]\n",
    "#         else:\n",
    "#             hot = [0 if nucleotide != nt else 1 for nt in nucleotides]\n",
    "\n",
    "#         one_hot.append(hot)\n",
    "#     return np.array(one_hot).flatten()\n",
    "#     # Apply flatten to make feature 2 Dimensional as Logistic Regression only support 2D matrix as features.\n",
    "\n",
    "\n",
    "# # Encode X Features Based on Middle Index\n",
    "# def encode_x_with_in_hot(x_train_raw):\n",
    "#     middle_index = (x_train_raw.shape[1] // 2) + 1\n",
    "#     truncated_df = x_train_raw.iloc[:, middle_index - WINDOW_SIZE - 1: middle_index + WINDOW_SIZE]\n",
    "#     print(truncated_df.shape)\n",
    "#     result = truncated_df.apply(apply_one_hot_encode, axis=1)\n",
    "#     return result \n",
    "\n",
    "# x_encoded = encode_x_with_in_hot(x_train_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode Y to its Original Form . Additional Class - NonMoD added for Non Modified RNA Sequence \n",
    "# RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
    "# RMEncoding = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "# def decode_y(row):   \n",
    "#     decoded = ''\n",
    "#     for index , n in enumerate(row.tolist()) :\n",
    "#         if n == 1 :\n",
    "#             decoded = RMs[index]\n",
    "    \n",
    "#     if decoded == '':\n",
    "#         return 'NonMoD'\n",
    "    \n",
    "#     return decoded \n",
    "\n",
    "# y_train_original =  y_train_raw.apply(decode_y , axis=1)\n",
    "# y_train_original.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Reference : https://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/train.py\n",
    "# x_train= x_encoded.tolist()\n",
    "# y_train = y_train_original \n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = RandomOverSampler(random_state=42 )\n",
    "# x_res, y_res = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "\n",
    "# print(y_res.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clf = OneVsRestClassifier(xgb.XGBClassifier(n_jobs=-1, max_depth=4 , objective='multi:softmax' , num_class = 13))\n",
    "# clf.fit(x_res, y_res )\n",
    "\n",
    "\n",
    "# xg_train = xgb.DMatrix(x_res, label=y_res)\n",
    "\n",
    "# # setup parameters for xgboost\n",
    "# param = {}\n",
    "# # use softmax multi-class classification\n",
    "# param['objective'] = 'multi:softmax'\n",
    "# # scale weight of positive examples\n",
    "# param['eta'] = 0.1\n",
    "# param['max_depth'] = 6\n",
    "# param['nthread'] = 4\n",
    "# param['num_class'] = 13\n",
    "\n",
    "\n",
    "# num_round = 5\n",
    "# bst = xgb.train(param, xg_train, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test=  y_test_raw.apply(decode_y , axis=1)\n",
    "\n",
    "# x_test = encode_x_with_in_hot(x_test_raw).tolist()\n",
    "# # xg_test = xgb.DMatrix(x_test)\n",
    "# # y_pred = bst.predict(xg_test)\n",
    "\n",
    "# y_pred = clf.predict(x_test)\n",
    "\n",
    "# # Calculate the accuracy score of the xgboost regression model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Accuracy of XGBoost  model: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot confusion matrix \n",
    "# cm = confusion_matrix(y_test, y_pred, labels=np.array(RMEncoding))\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "#                               display_labels=np.array(RMEncoding))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# disp.plot(ax=ax)\n",
    "\n",
    "# print(\"Accuracy of XGBoost  model: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# svm_model_linear = SVC(kernel = 'linear', C = 1).fit(x_res, y_res)\n",
    "# svm_predictions = svm_model_linear.predict(x_test)\n",
    "  \n",
    "# # model accuracy for X_test  \n",
    "# accuracy = svm_model_linear.score(x_test, y_test)\n",
    "  \n",
    "# # creating a confusion matrix\n",
    "# cm = confusion_matrix(y_test, svm_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, input_seq):\n",
    "#         # Initialize hidden state and cell state with zeros\n",
    "#         h0 = torch.zeros(1, input_seq.size(1), self.hidden_size).to(device)\n",
    "#         c0 = torch.zeros(1, input_seq.size(1), self.hidden_size).to(device)\n",
    "\n",
    "#         # Forward pass through LSTM layer\n",
    "#         output, (hidden, cell) = self.lstm(input_seq, (h0, c0))\n",
    "\n",
    "#         # Get the final output of the LSTM (last hidden state)\n",
    "#         final_output = hidden[-1, :, :]\n",
    "\n",
    "#         # Forward pass through fully connected layer\n",
    "#         output = self.fc(final_output)\n",
    "\n",
    "#         # Softmax activation function to get probabilities\n",
    "#         output = nn.functional.softmax(output, dim=1)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # # RNA sequences (input)\n",
    "# # train_data = [\n",
    "# #     'AUUGGCGCUUAC',\n",
    "# #     'AGCUGGGAGUUCAA',\n",
    "# #     'AUCUCAGGACAGUUGGCUUUGUUUGUCC',\n",
    "# #     # ...\n",
    "# # ]\n",
    "\n",
    "# # Target labels (output)\n",
    "# train_labels = [\n",
    "#     1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n",
    "# ]\n",
    "\n",
    "# # One-hot encoding of RNA sequences\n",
    "# #base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "# input_dim = 404  # A, C, G, \n",
    "# max_len = 1001\n",
    "\n",
    "# train_data_oh = torch.from_numpy(X_train)\n",
    "\n",
    "\n",
    "# # Define LSTM model\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, input_seq):\n",
    "#         lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
    "#         last_out = lstm_out[-1]\n",
    "#         out = self.fc(last_out)\n",
    "#         return out\n",
    "\n",
    "# # Initialize model and optimizer\n",
    "# hidden_dim = 16\n",
    "# output_dim = 13\n",
    "# model = LSTMModel(input_dim, hidden_dim, output_dim)\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# # Train model\n",
    "# num_epochs = 100\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # for i in range(len(train_data_oh)):\n",
    "#     #     input_seq = train_data_oh[i]\n",
    "#     label = torch.tensor([train_labels[i]], dtype=torch.long)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(train_data_oh)\n",
    "#     loss = loss_fn(output, label)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if epoch % 10 == 0:\n",
    "#         print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# # Test model\n",
    "# test_data = [\n",
    "#     'UCCUGGGUCCAGUUCUGCUUAAACUUAGUUGCCUAA',\n",
    "#     'AGCCUCUUGGGAUUGGGCUUUGUUUGUCC',\n",
    "#     # ...\n",
    "# ]\n",
    "# MAX_SEQ_LEN = 200\n",
    "# test_data_oh = torch.zeros(len(test_data), MAX_SEQ_LEN, len(base_to_idx))\n",
    "# for i, seq in enumerate(test_data):\n",
    "#     for j, base in enumerate(seq):\n",
    "#         test_data_oh[i, j, base_to_idx[base]] = 1\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(test_data)):\n",
    "#         input_seq = test_data_oh[i]\n",
    "#         output = model(input_seq)\n",
    "#         predicted_label = torch.argmax(output).item()\n",
    "#         print('Test example {}: predicted label {}'.format(i+1, predicted_label))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA_ModX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c230947687a8f3ffad2ce5baec6aac89e01c839661a42aacfb7c80a5442a49ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
