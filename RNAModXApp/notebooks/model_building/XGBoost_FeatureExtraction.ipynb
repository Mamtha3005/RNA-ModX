{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    " #Import All Libraries Here\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split , RandomizedSearchCV\n",
    "import xgboost as xgb \n",
    "import statistics\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from igraph import *\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "\n",
    "# PyTorch Import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Constants\n",
    "INPUT_TRAIN_IN = \"../../data/train_in.csv\"\n",
    "INPUT_TRAIN_OUT = \"../../data/train_out.csv\"  \n",
    "INPUT_TEST_IN = \"../../data/test_in.csv\"\n",
    "INPUT_TEST_OUT = \"../../data/test_out.csv\"\n",
    "INPUT_VALIDATION_IN = \"../../data/valid_in_nucleo.csv\"\n",
    "INPUT_VALIDATION_OUT  = \"../../data/valid_out.csv\"\n",
    "\n",
    "TARGET_MODEL_PATH = '../../webapp/model_files'\n",
    "\n",
    "\n",
    "WINDOW_SIZE =  10 # Final RNA Sequence will be 101 Length \n",
    "MAX_LENGTH = (WINDOW_SIZE*2) +1\n",
    "\n",
    "\n",
    "# 1 - ANF Encoding \n",
    "# 2 - One Hot Encoding  \n",
    "# 3 - Complex Network  \n",
    "# 4 - Word2Vec Model \n",
    "# 5 - K-mer with One Hot \n",
    "# 6 - Multi RM Encoding\n",
    "# 7 - Multi Dimensional K-mer with One Hot \n",
    "ENCODING_METHOD = 3\n",
    "\n",
    "# 1 - XGBOOST \n",
    "# 2 - XGBOOST WITH Randomize Search \n",
    "# 3 - XGBOOST WITH Grid Search \n",
    "# 4 - Random Forest \n",
    "# 5 - XGBOOST WITH Optuna Hyper Parameter Optimizatio\n",
    "# 6 - LSTM \n",
    "MODEL = 6\n",
    "\n",
    "FRAMEWORK = \"PYTORCH\"\n",
    "\n",
    "INPUT_DIMENSION = 0\n",
    "\n",
    "PERFORM_DATA_BALANCING = True\n",
    "K_MERS_SIZE = 3\n",
    "\n",
    "\n",
    "# Startegy to Crop Sequene\n",
    "# MID - Modification is present at Mid of cropped Sequence \n",
    "# END - Modification is present at End of cropepd Sequence \n",
    "CROP_STRATEGY = 'END'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape of X : (304661, 1001) and Tranin Shape of Y : (304661, 1001)\n",
      "Test Shape of X : (1200, 1001) and Test Shape of Y : (1200, 12)\n",
      "Validation Shape of X : (3599, 1001) and Validation Shape of Y : (3599, 12)\n",
      "Shape of X : (309460, 1001) and Shape of Y : (309460, 12)\n"
     ]
    }
   ],
   "source": [
    "#Read X Varaibles and Y Varaibles\n",
    "\n",
    "x_train_raw =  pd.read_csv(INPUT_TRAIN_IN, header=None , skiprows=1)\n",
    "y_train_raw =  pd.read_csv(INPUT_TRAIN_OUT, header=None , skiprows=1 )\n",
    "\n",
    "x_test_raw =  pd.read_csv(INPUT_TEST_IN, header=None , skiprows=1 )\n",
    "y_test_raw =  pd.read_csv(INPUT_TEST_OUT, header=None , skiprows=1)\n",
    "\n",
    "x_valid_raw =  pd.read_csv(INPUT_VALIDATION_IN, header=None , skiprows=1 )\n",
    "y_valid_raw =  pd.read_csv(INPUT_VALIDATION_OUT, header=None , skiprows=1 )\n",
    "\n",
    "x_data = pd.concat([x_train_raw, x_test_raw, x_valid_raw], axis=0, ignore_index=True)\n",
    "y_data = pd.concat([y_train_raw, y_test_raw, y_valid_raw], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Train Shape of X : {x_train_raw.shape} and Tranin Shape of Y : {x_train_raw.shape}\")\n",
    "print(f\"Test Shape of X : {x_test_raw.shape} and Test Shape of Y : {y_test_raw.shape}\")\n",
    "print(f\"Validation Shape of X : {x_valid_raw.shape} and Validation Shape of Y : {y_valid_raw.shape}\")\n",
    "print(f\"Shape of X : {x_data.shape} and Shape of Y : {y_data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sequence Positions to extracted from Original Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : (309460, 1001) and Shape of Y : (309460, 12)\n"
     ]
    }
   ],
   "source": [
    "middle_index = (x_train_raw.shape[1] // 2) + 1\n",
    "\n",
    "if CROP_STRATEGY == 'MID':\n",
    "    STRAT_INEDX =middle_index - WINDOW_SIZE -1 \n",
    "    END_INDEX =middle_index + WINDOW_SIZE \n",
    "\n",
    "if CROP_STRATEGY == 'END':\n",
    "    STRAT_INEDX =middle_index - (WINDOW_SIZE*2) -1 \n",
    "    END_INDEX =middle_index\n",
    "\n",
    "# Testing data sample\n",
    "# x_data =x_data.iloc[:1000]\n",
    "# y_data =y_data.iloc[:1000]\n",
    "\n",
    "print(f\"Shape of X : {x_data.shape} and Shape of Y : {y_data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Graph - Complex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_graph(thresholdCN):\n",
    "    metrics = []\n",
    "    metrics.append(mean(thresholdCN.betweenness(directed=False, weights=None)))\n",
    "    metrics.append(mean(thresholdCN.degree()))\n",
    "    metrics.append(thresholdCN.assortativity_degree(directed=False))  # Returns the assortativity\n",
    "    metrics.append(max(thresholdCN.degree()))\n",
    "    metrics.append(min(thresholdCN.degree()))\n",
    "    metrics.append(np.std(thresholdCN.degree()))  # Returns the strength (weighted degree)\n",
    "    metrics.append(thresholdCN.average_path_length(directed=False, unconn=False))  # Average path length\n",
    "    metrics.append(thresholdCN.transitivity_avglocal_undirected())  # local transitivity (clustering coefficient)\n",
    "    metrics.append(thresholdCN.transitivity_undirected())  # global transitivity (clustering coefficient)\n",
    "    metrics.append(cn.ecount())  # Counts the number of edges\n",
    "    metrics.append(thresholdCN.motifs_randesu_no(size=3))\n",
    "    metrics.append(thresholdCN.motifs_randesu_no(size=4))\n",
    "    metrics.append(mean(thresholdCN.authority_score()))\n",
    "    metrics.append(mean(thresholdCN.closeness(vertices=None, mode=ALL, cutoff=None, weights=None, normalized=True)))  # Calculates the closeness centralities of given vertices in a graph\n",
    "    metrics.append(mean(thresholdCN.constraint(vertices=None, weights=None)))  # Calculates Burt's constraint scores for given vertices in a graph.\n",
    "    metrics.append(mean(thresholdCN.count_multiple(edges=None)))  # Counts the multiplicities of the given edges.\n",
    "    metrics.append(thresholdCN.density(loops=False))  # Calculates the density of the graph.\n",
    "    metrics.append(thresholdCN.diameter(directed=False, unconn=False, weights=None))  # Calculates the diameter of the graph.\n",
    "    metrics.append(mean(thresholdCN.eccentricity(vertices=None, mode=ALL)))  # Calculates the eccentricities of given vertices in a graph.\n",
    "    metrics.append(mean(thresholdCN.edge_betweenness(directed=False, cutoff=None, weights=None)))  # Calculates or estimates the edge betweennesses in a graph.\n",
    "    metrics.append(mean(thresholdCN.hub_score()))  # Calculates Kleinberg's hub score for the vertices of the graph.\n",
    "    metrics.append(thresholdCN.maxdegree())  # Returns the maximum degree of a vertex set in the graph.\n",
    "    metrics.append(mean(thresholdCN.neighborhood_size()))  # For each vertex specified by vertices, returns the number of vertices reachable from that vertex in at most order steps\n",
    "    metrics.append(thresholdCN.radius())  # Calculates the radius of the graph.\n",
    "    metrics.append(mean(thresholdCN.strength()))  # Returns the strength (weighted degree) of some vertices from the graph.\n",
    "    metrics.append(cn.vcount())  # Counts the number of vertices.\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def patterns(seq, win):\n",
    "    \"\"\"\n",
    "    Generate k-mers: subsequences of length k\n",
    "    contained in a biological sequence.\n",
    "    \"\"\"\n",
    "    seqlen = len(seq)\n",
    "    for i in range(seqlen):\n",
    "        j = seqlen if i + win > seqlen else i + win\n",
    "        yield seq[i:j]\n",
    "        if j == seqlen: break\n",
    "    return\n",
    "\n",
    "def complex_network(seq):\n",
    "    \"\"\"Generates complex network\"\"\"\n",
    "    global name_seq, cn\n",
    "    ksize = K_MERS_SIZE\n",
    "    threshold = 3\n",
    "\n",
    "    metrics = []\n",
    "    cn = Graph()\n",
    "    seq = seq.upper()\n",
    "    \n",
    "    for k in range(1, ksize + 1):\n",
    "        cn = Graph()\n",
    "        kmer = []\n",
    "        for subseq in patterns(seq, k):  # Generates k pattern\n",
    "            kmer.append(str(subseq))\n",
    "        #print(kmer)\n",
    "        vertices = np.unique(kmer)\n",
    "        for vert in vertices:\n",
    "            cn.add_vertices(vert)\n",
    "        for i in range(len(kmer)-1):  # Position -1 -- Build the Network\n",
    "            cn.add_edges([(kmer[i], kmer[i+1])])\n",
    "        # print(summary(cn))\n",
    "        metrics +=feature_extraction_graph(cn)\n",
    "        #print('Result Matric' , metrics)\n",
    "        #print('Size of Matric with k ' , k , '  Size ' , len(metrics))\n",
    "    metrics_preprocessing = np.nan_to_num(metrics)     \n",
    "    return metrics_preprocessing\n",
    "\n",
    "def encode_using_graph(x_train_raw):\n",
    "    truncated_df = x_train_raw.iloc[:,STRAT_INEDX :END_INDEX]\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    result = truncated_df['Sequence'].apply(complex_network)\n",
    "    return result \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply ANF Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ANF Encoding on Input Features\n",
    "def apply_accumulated_nucle_frequency(seq):\n",
    "    mapping = []\n",
    "    A = 0\n",
    "    C = 0\n",
    "    T = 0\n",
    "    G = 0\n",
    "    for i, v in seq.items():\n",
    "        if v == 'A':\n",
    "            A += 1\n",
    "            mapping.append(A / (i + 1))\n",
    "        elif v == 'C':\n",
    "            C += 1\n",
    "            mapping.append(C / (i + 1))\n",
    "        elif v == 'T' or v == 'U':\n",
    "            T += 1\n",
    "            mapping.append(T / (i + 1))\n",
    "        else:\n",
    "            G += 1\n",
    "            mapping.append(G / (i + 1))\n",
    "    padding = (MAX_LENGTH - len(mapping))\n",
    "    mapping = np.pad(mapping, (0, padding), 'constant')\n",
    "    return mapping\n",
    "\n",
    "# # Encode X Features Based on Middle Index\n",
    "def encode_using_anf(x_train_raw):\n",
    "    truncated_df = x_train_raw.iloc[:,STRAT_INEDX :END_INDEX]\n",
    "    print(truncated_df.shape)\n",
    "    result = truncated_df.apply(apply_accumulated_nucle_frequency, axis=1)\n",
    "    return result "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One Hot Encoding for RNA Sequence \n",
    "def one_hot_encode_sequence(seq):\n",
    "    bases = 'ACGT'\n",
    "    base_dict = {base: i for i, base in enumerate(bases)}\n",
    "    one_hot = np.zeros((len(seq), len(bases)))\n",
    "    for i, base in enumerate(seq):\n",
    "        if base == 'N':\n",
    "            continue\n",
    "        one_hot[i, base_dict[base]] = 1\n",
    "    return one_hot.flatten()\n",
    "\n",
    "\n",
    "# Encode X Features Based on Middle Index\n",
    "def encode_x_with_in_hot(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    concatenated_column= truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    df_result = truncated_df.assign(Sequence=concatenated_column)\n",
    "    result = df_result['Sequence'].apply(one_hot_encode_sequence)\n",
    "    return result \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Documentation - \n",
    "# Encode RNA sequences using Word2Vec embeddings\n",
    "def encode_sequence(sequence , word2vec_model):\n",
    "    embeddings = []\n",
    "    for kmer in sequence:\n",
    "        embeddings.append(word2vec_model.wv[kmer])\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def applyKmersAndEncoding(seq):\n",
    "    k = 3\n",
    "    tokenized_sequences = [''.join(seq[i:i+k]) for i in range(0, len(seq)-k+1)]\n",
    "    return tokenized_sequences\n",
    "    \n",
    "\n",
    "\n",
    "def encode_x_with_word2Vec(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    tokenized_sequences =  truncated_df['Sequence'].apply(applyKmersAndEncoding).tolist()\n",
    "    \n",
    "    # The result, tokenized_sequences, is a list of lists, where each inner list\n",
    "    #  contains the k-mers of the corresponding RNA sequence from the truncated_df list. \n",
    "    embedding_size = 100\n",
    "    word2vec_model = gensim.models.Word2Vec(tokenized_sequences, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "    result = []\n",
    "    for seq in tokenized_sequences:\n",
    "        embedding = encode_sequence(seq , word2vec_model)\n",
    "        result.append(embedding)\n",
    "    return result \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply One Hot with K mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyOneHotEncoding(tokenized_sequences):\n",
    "    encoded_sequences = []\n",
    "    for seq in tokenized_sequences:\n",
    "        encoded_sequences.append(one_hot_encode_sequence(seq))    \n",
    "    # print(\"Encoded Sequence \")\n",
    "    # print(encoded_sequences)\n",
    "    return np.array(encoded_sequences).flatten()\n",
    "\n",
    "def applyKmersAndEncoding(seq):\n",
    "    k=3\n",
    "    tokens = [seq[i:i+k] for i in range(0, len(seq)-k+1)]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def encode_x_with_k_mer_one_hot_encoding(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    tokenized_sequences =  truncated_df['Sequence'].apply(applyKmersAndEncoding).tolist()\n",
    "    \n",
    "    # The result, tokenized_sequences, is a list of lists, where each inner list\n",
    "    #  contains the k-mers of the corresponding RNA sequence from the truncated_df list. \n",
    "   \n",
    "\n",
    "    result = []\n",
    "    for seq in tokenized_sequences:\n",
    "        embedding = applyOneHotEncoding(seq)\n",
    "        result.append(embedding)\n",
    "    return np.array(result) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Multi RM K-mers Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"C:/Users/shashi.vish/Documents/Shashi/Education/HigherEducation/NUS/Capstone Project/Git\\MultiRM/Embeddings/embeddings_12RM.pkl\", 'rb'))\n",
    "\n",
    "def applyMultiRmEncoding(tokenized_sequences):\n",
    "    embeddings = []\n",
    "    for kmer in tokenized_sequences:\n",
    "        embeddings.append(model[kmer])\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def applyKmersAndEncoding(seq):\n",
    "    k=3\n",
    "    tokens = [seq[i:i+k] for i in range(0, len(seq)-k+1)]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def encode_x_with_k_mer_multi_rm(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    tokenized_sequences =  truncated_df['Sequence'].apply(applyKmersAndEncoding).tolist()\n",
    "\n",
    "    result = []\n",
    "    for seq in tokenized_sequences:\n",
    "        embedding = applyMultiRmEncoding(seq)\n",
    "        result.append(embedding)\n",
    "    return result \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Encoding of X Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_41816\\1323896942.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truncated_df['Sequence'] = truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Encoding Method\n",
    "if ENCODING_METHOD == 1:\n",
    "    x_encoded = encode_using_anf(x_data).tolist()\n",
    "if ENCODING_METHOD == 3:\n",
    "    x_encoded = encode_using_graph(x_data).tolist()\n",
    "    #x_encoded = encode_using_graph(x_data).tolist()\n",
    "if ENCODING_METHOD == 2:\n",
    "    x_train = pd.DataFrame()\n",
    "    x_train['one_hot_encoded'] = encode_x_with_in_hot(x_data)\n",
    "    x_encoded = np.array(x_train['one_hot_encoded'].tolist())\n",
    "    INPUT_DIMENSION = 4\n",
    "if ENCODING_METHOD == 4:\n",
    "     x_encoded = encode_x_with_word2Vec(x_data)\n",
    "if ENCODING_METHOD == 5:\n",
    "     x_encoded = encode_x_with_k_mer_one_hot_encoding(x_data)     \n",
    "if ENCODING_METHOD == 6:\n",
    "     x_encoded = encode_x_with_k_mer_multi_rm(x_data)   \n",
    "\n",
    "\n",
    "\n",
    "# print(\"Shape of Encoded Data : \" ,x_encoded.shape)\n",
    "# print(\"Sample Records : \", x_encoded[0])  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist Feature to a File.\n",
    "# import csv\n",
    "# with open('Complext_Network_Feature_Validation.csv', 'w', newline='') as csvfile:\n",
    "#     writer = csv.writer(csvfile , delimiter = ',')\n",
    "#     for row in x_encoded:\n",
    "#         writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Each Y category to 0 to 12 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     154607\n",
       "7      65178\n",
       "11     52618\n",
       "4      16341\n",
       "6       3696\n",
       "5       3207\n",
       "10      3137\n",
       "8       2447\n",
       "3       2253\n",
       "1       1878\n",
       "12      1591\n",
       "2       1471\n",
       "9       1036\n",
       "dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decode Y to its Original Form . Additional Class - NonMoD added for Non Modified RNA Sequence \n",
    "RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
    "RMEncoding = [12,1,2,3,4,5,6,7,8,9,10,11,0]\n",
    "\n",
    "def decode_y(row):   \n",
    "    decoded = ''\n",
    "    for index , n in enumerate(row.tolist()) :\n",
    "        if n == 1 :\n",
    "            decoded = RMEncoding[index]\n",
    "    \n",
    "    if decoded == '':\n",
    "        return 0\n",
    "    \n",
    "    return decoded \n",
    "\n",
    "y_encoded = y_data.apply(decode_y,axis=1)\n",
    "\n",
    "y_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Reference : https://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/train.py\n",
    "\n",
    "# PERFORM_DATA_BALANCING - if True then code will perform over sampling for each category.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X :  torch.Size([19, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(x_encoded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Test and Validation set\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "y_train.value_counts()\n",
    "\n",
    "# Transform into Tensor\n",
    "if FRAMEWORK == 'PYTORCH':\n",
    "    X_train = torch.tensor(X_train , dtype=torch.float)\n",
    "    y_train = torch.tensor(np.array(y_train), dtype=torch.long)  # Cross Entropy Expects Long Data Type\n",
    "\n",
    "    X_test = torch.tensor(X_test , dtype=torch.float)\n",
    "    y_test = torch.tensor(np.array(y_test) , dtype=torch.long)\n",
    "\n",
    "    X_valid = torch.tensor(X_valid , dtype=torch.float)\n",
    "    y_valid = torch.tensor(np.array(y_valid)  , dtype=torch.long)\n",
    "\n",
    "    print(\"Shape of X : \" , X_train[0].shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Sample Data Using RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORM_DATA_BALANCING:\n",
    "    sm = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "    print(y_resampled.value_counts())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if MODEL ==1:\n",
    "    # Define XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 13,\n",
    "        'max_depth': 4,\n",
    "        'n_jobs': -1,\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "\n",
    "\n",
    "    if PERFORM_DATA_BALANCING:\n",
    "        dtrain = xgb.DMatrix(X_resampled, label=y_resampled)\n",
    "        num_rounds = 100    \n",
    "        model = xgb.train(params, dtrain, num_rounds)     \n",
    "    else:\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        num_rounds = 100    \n",
    "        model = xgb.train(params, dtrain, num_rounds) \n",
    "\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_pred = model.predict(dtest) \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "    dvalid = xgb.DMatrix(X_valid)\n",
    "    # Make predictions on test set\n",
    "    y_pred_val = model.predict(dvalid) \n",
    "    accuracy = accuracy_score(y_valid, y_pred_val)\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "    # Plot confusion matrix \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.array(RMEncoding))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=np.array(RMEncoding))\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    disp.plot(ax=ax)\n",
    "\n",
    "    print(\"Accuracy of XGBoost  model: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XgBoost with Hyper Paramter Tuning - RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 2:\n",
    "    # Create the XGBoost classifier\n",
    "    xgb_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=13, random_state=42)\n",
    "\n",
    "    # Define the hyperparameter search space\n",
    "    param_dist = {\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_child_weight':   [1, 3, 5],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "        'colsample_bytree': [0.5, 0.75, 1],\n",
    "        'n_estimators': [100, 250, 500]\n",
    "    }\n",
    "\n",
    "    # Create the RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(xgb_clf, param_dist, cv=3, n_iter=20, scoring='accuracy', verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "    # Perform the randomized search\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"Best hyperparameters:\\n\", best_params)\n",
    "\n",
    "    # Train the classifier with the best hyperparameters\n",
    "    best_xgb_clf = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=13, random_state=42)\n",
    "    best_xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_xgb_clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XgBoost with Hyper Paramter Tuning - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "if MODEL == 3:\n",
    "    # Create the XGBoost classifier\n",
    "    xgb_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=13, random_state=42 )\n",
    "\n",
    "    # Define the hyperparameter search space\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "        'colsample_bytree': [0.5, 0.75, 1],\n",
    "        'n_estimators': [100, 250, 500]\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(xgb_clf, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best hyperparameters:\\n\", best_params)\n",
    "\n",
    "    # Train the classifier with the best hyperparameters\n",
    "    best_xgb_clf = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=13, random_state=42)\n",
    "    best_xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_xgb_clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 4:\n",
    "    # Train the Random Forest classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with Optuna With XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": 13 ,\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n",
    "    return accuracy\n",
    "\n",
    "if MODEL == 5:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=6000)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# Number of finished trials:  76\n",
    "# Best trial:\n",
    "#   Value: 0.6052047652900752\n",
    "#   Params: \n",
    "#     booster: gblinear\n",
    "#     lambda: 3.0789981964094497e-06\n",
    "#     alpha: 2.468290506036041e-06\n",
    "#     subsample: 0.8524524714955344\n",
    "#     colsample_bytree: 0.498783948236617"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# TARGET_MODEL_PATH += '/xgboost_model.bin'\n",
    "\n",
    "# ##dump the model into a file\n",
    "# with open(TARGET_MODEL_PATH, 'wb') as f_out:\n",
    "#     pickle.dump(bst, f_out) # write final_model in .bin file\n",
    "#     f_out.close()  # close the file \n",
    "#     print(\"Exported Model Successfully\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA_ModX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c230947687a8f3ffad2ce5baec6aac89e01c839661a42aacfb7c80a5442a49ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
