{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import All Libraries Here\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split , RandomizedSearchCV\n",
    "import xgboost as xgb \n",
    "import statistics\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from igraph import *\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "\n",
    "# PyTorch Import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Constants\n",
    "INPUT_TRAIN_IN = \"../../data/train_in.csv\"\n",
    "INPUT_TRAIN_OUT = \"../../data/train_out.csv\"  \n",
    "INPUT_TEST_IN = \"../../data/test_in.csv\"\n",
    "INPUT_TEST_OUT = \"../../data/test_out.csv\"\n",
    "INPUT_VALIDATION_IN = \"../../data/valid_in_nucleo.csv\"\n",
    "INPUT_VALIDATION_OUT  = \"../../data/valid_out.csv\"\n",
    "\n",
    "TARGET_MODEL_PATH = '../../webapp/model_files'\n",
    "\n",
    "\n",
    "WINDOW_SIZE =  50 # Final RNA Sequence will be 101 Length \n",
    "MAX_LENGTH = (WINDOW_SIZE*2) +1\n",
    "\n",
    "# 1 - Multi Dimensional K-mer with One Hot \n",
    "ENCODING_METHOD = 1\n",
    "\n",
    "# 1 - LSTM with Cross Entropy \n",
    "MODEL = 1\n",
    "\n",
    "FRAMEWORK = \"PYTORCH\"\n",
    "\n",
    "INPUT_DIMENSION = 0\n",
    "\n",
    "PERFORM_DATA_BALANCING = False\n",
    "K_MERS_SIZE = 3\n",
    "\n",
    "\n",
    "# Startegy to Crop Sequene\n",
    "# MID - Modification is present at Mid of cropped Sequence \n",
    "# END - Modification is present at End of cropepd Sequence \n",
    "CROP_STRATEGY = 'END'\n",
    "\n",
    "# Y Category Encoding Method\n",
    "# LABEL or ONE_HOT \n",
    "TARGET_ENCODING = 'ONE_HOT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : (309460, 1001) and Shape of Y : (309460, 12)\n"
     ]
    }
   ],
   "source": [
    "#Read X Varaibles and Y Varaibles\n",
    "\n",
    "x_train_raw =  pd.read_csv(INPUT_TRAIN_IN, header=None , skiprows=1 )\n",
    "y_train_raw =  pd.read_csv(INPUT_TRAIN_OUT, header=None , skiprows=1)\n",
    "\n",
    "x_test_raw =  pd.read_csv(INPUT_TEST_IN, header=None , skiprows=1 )\n",
    "y_test_raw =  pd.read_csv(INPUT_TEST_OUT, header=None , skiprows=1)\n",
    "\n",
    "x_valid_raw =  pd.read_csv(INPUT_VALIDATION_IN, header=None , skiprows=1 )\n",
    "y_valid_raw =  pd.read_csv(INPUT_VALIDATION_OUT, header=None , skiprows=1 )\n",
    "\n",
    "x_data = pd.concat([x_train_raw, x_test_raw, x_valid_raw], axis=0, ignore_index=True)\n",
    "y_data = pd.concat([y_train_raw, y_test_raw, y_valid_raw], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "middle_index = (x_train_raw.shape[1] // 2) + 1\n",
    "\n",
    "if CROP_STRATEGY == 'MID':\n",
    "    STRAT_INEDX =middle_index - WINDOW_SIZE -1 \n",
    "    END_INDEX =middle_index + WINDOW_SIZE \n",
    "\n",
    "if CROP_STRATEGY == 'END':\n",
    "    STRAT_INEDX =middle_index - (WINDOW_SIZE*2) -1 \n",
    "    END_INDEX =middle_index\n",
    "\n",
    "# Testing data sample\n",
    "# x_data =x_data.iloc[:10000]\n",
    "# y_data =y_data.iloc[:10000]\n",
    "\n",
    "print(f\"Shape of X : {x_data.shape} and Shape of Y : {y_data.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Dimensional Kmers with One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_unique_kmers = set()\n",
    "def encode_seq(kmer_token):\n",
    "\n",
    "    # A 1 0 0 0\n",
    "    # C 0 1 0 0\n",
    "    # T/U 0 0 0 1\n",
    "    # G 0 0 1 0\n",
    "    # N 0 0 0 0\n",
    "\n",
    "    encoding_dict = {\n",
    "        'A': [1, 0, 0, 0],\n",
    "        'C': [0, 1, 0, 0],\n",
    "        'G': [0, 0, 1, 0],\n",
    "        'T': [0, 0, 0, 1],\n",
    "        'U': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0],\n",
    "    }\n",
    "\n",
    "    encoded_sequence = []\n",
    "    #print(\"Input Sequence : \", kmer_token)\n",
    "    number_of_unique_kmers.add(kmer_token)\n",
    "    for  base in kmer_token:\n",
    "        #print(\"Base : \" , base)\n",
    "        encoded_sequence.append(encoding_dict[base])\n",
    "    return np.array(encoded_sequence).flatten()\n",
    "\n",
    "\n",
    "    # bases = 'ACGT'\n",
    "    # base_dict = {base: i for i, base in enumerate(bases)}\n",
    "    # one_hot = np.zeros((len(seq), len(bases)))\n",
    "    # for i, base in enumerate(seq):\n",
    "    #     if base == 'N':\n",
    "    #         continue\n",
    "    #     one_hot[i, base_dict[base]] = 1\n",
    "    # return one_hot.flatten()\n",
    "\n",
    "def applyOneHotEncoding(tokenized_sequences):\n",
    "    encoded_sequences = []\n",
    "    for seq in tokenized_sequences:\n",
    "        encoded_sequences.append(encode_seq(seq)) \n",
    " \n",
    "    return np.array(encoded_sequences)\n",
    "\n",
    "def applyKmersAndEncoding(seq):\n",
    "    k=3\n",
    "    tokens = [seq[i:i+k] for i in range(0, len(seq)-k+1)]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "def encode_x_with_mult_dimensional_k_mer_one_hot_encoding(x_train_raw):\n",
    "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
    "    concatenated_column= truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
    "    df_result = truncated_df.assign(Sequence=concatenated_column)\n",
    "    tokenized_sequences =  df_result['Sequence'].apply(applyKmersAndEncoding).tolist()\n",
    "    \n",
    "    # The result, tokenized_sequences, is a list of lists, where each inner list\n",
    "    #  contains the k-mers of the corresponding RNA sequence from the truncated_df list. \n",
    "   \n",
    "    result = []\n",
    "    for seq in tokenized_sequences:\n",
    "        embedding = applyOneHotEncoding(seq)\n",
    "        result.append(embedding)\n",
    "    return np.array(result) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Encoded Data :  (309460, 99, 12)\n",
      "Sample Records :  [[0 0 1 ... 0 1 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 1 ... 0 1 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 1]\n",
      " [1 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Total Unique K mers :  64\n"
     ]
    }
   ],
   "source": [
    "if ENCODING_METHOD == 1:\n",
    "    x_encoded=encode_x_with_mult_dimensional_k_mer_one_hot_encoding(x_data)\n",
    "\n",
    "print(\"Shape of Encoded Data : \" ,x_encoded.shape)\n",
    "print(\"Sample Records : \", x_encoded[0])  \n",
    "print(\"Total Unique K mers : \" , len(number_of_unique_kmers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Decode Y to its Original Form . Additional Class - NonMoD added for Non Modified RNA Sequence \n",
    "RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
    "RMEncoding = [12,1,2,3,4,5,6,7,8,9,10,11,0]\n",
    "\n",
    "\n",
    "def convert_y_to_original_labels(row):\n",
    "    label = \"\"\n",
    "    for index , n in enumerate(row.tolist()) :\n",
    "        if n == 1 :\n",
    "            label = RMs[index]\n",
    "    if label == '':\n",
    "        return 'NonMoD'\n",
    "    \n",
    "    return label\n",
    "\n",
    "# Convert One Hot Encoded Y to to Original Labels \n",
    "y_original_labels = y_data.apply(convert_y_to_original_labels,axis=1)\n",
    "\n",
    "if TARGET_ENCODING  == 'LABEL':\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_original_labels)\n",
    "    y_encoded = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "if TARGET_ENCODING == 'ONE_HOT':\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    y_encoded = ohe.fit_transform(y_original_labels.to_numpy().reshape(-1, 1))\n",
    "    y_encoded = torch.tensor(y_encoded, dtype=torch.float32)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X :  torch.Size([99, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(x_encoded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Test and Validation set\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Transform into Tensor\n",
    "if FRAMEWORK == 'PYTORCH':\n",
    "    X_train = torch.tensor(X_train , dtype=torch.float)\n",
    "    X_test = torch.tensor(X_test , dtype=torch.float)\n",
    "    X_valid = torch.tensor(X_valid , dtype=torch.float)\n",
    "\n",
    "    if TARGET_ENCODING == 'LABEL':\n",
    "        y_train= torch.tensor(np.array(y_train), dtype=torch.long)  # Cross Entropy Expects Long Data Type\n",
    "        y_test = torch.tensor(np.array(y_test) , dtype=torch.long)\n",
    "        y_valid = torch.tensor(np.array(y_valid)  , dtype=torch.long)\n",
    "\n",
    "    if TARGET_ENCODING == 'ONE_HOT':\n",
    "        y_train= torch.tensor(np.array(y_train), dtype=torch.float)  # Binary Cross Entropy Expects Float Data Type\n",
    "        y_test = torch.tensor(np.array(y_test) , dtype=torch.float)\n",
    "        y_valid = torch.tensor(np.array(y_valid)  , dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Shape of X : \" , X_train[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORM_DATA_BALANCING:\n",
    "    sm = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "    print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Hyper Paramerts \n",
    "INPUT_DIMENSION = len(number_of_unique_kmers)\n",
    "HIDDEN_DIMENSION = 256\n",
    "BATCH_SIZE = 36\n",
    "OUTPUT_DIMENSION = 13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model with Cross Entropy Loss (Works with Label Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define LSTM and Train\n",
    "\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "class RNAClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNAClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        out = self.fc(last_output)\n",
    "        return out\n",
    "         \n",
    "if MODEL==1 :\n",
    "    print(\"Excuting LSTM Model\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "\n",
    "\n",
    "    print(\"Shape of Tensor : \" , X_train.shape)\n",
    "    train_dataset = RNADataset(X_train, y_train)\n",
    "    test_dataset = RNADataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Calculating INPUT_DIMENSION for LSTM Mode \n",
    "    # If you are just using One Hot then INPUT_DIMENSION would be 4 because there are 4 possible unique nucleosides\n",
    "    # But if you are using K mers then INPUT_DIMENSION would be number of unique k-mers on all dataset\n",
    "\n",
    "    # LSTM model Format - (batch_size, sequence_length, input_size)  \n",
    "    # batch_size comes from DataLoaded , you can tune it.\n",
    "    # Sequence Length - Lenth of X[0] shape (32, 19, 12)  Here 19 is sequence length , \n",
    "    # Input Size = 12 length of X[0][0] - Basically all unique nucleoside or k-mers\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = RNAClassifier(input_dim=12, hidden_dim=256, output_dim=13)\n",
    "    criterion = nn.CrossEntropyLoss()  ## MSELoss of Regression problem \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Number of Parameters for Model\n",
    "    total_parameters = []\n",
    "    for p in model.parameters():\n",
    "        total_parameters.append(p.numel())\n",
    "    \n",
    "    print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model with Binary Cross Entropy Loss (Works with One Hot Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excuting LSTM Model\n",
      "Shape of Tensor :  torch.Size([216622, 99, 12])\n",
      "Total Number of Parameters for Model Training : 279821 \n",
      "Epoch 1, Loss: 0.1377\n"
     ]
    }
   ],
   "source": [
    "## Define LSTM and Train\n",
    "\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "class RNAClassifier_BCE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNAClassifier_BCE, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden.squeeze(0))\n",
    "        return torch.softmax(x, dim=-1) \n",
    "         \n",
    "if MODEL==1 :\n",
    "    print(\"Excuting LSTM Model\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "\n",
    "\n",
    "    print(\"Shape of Tensor : \" , X_train.shape)\n",
    "    train_dataset = RNADataset(X_train, y_train)\n",
    "    test_dataset = RNADataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Calculating INPUT_DIMENSION for LSTM Mode \n",
    "    # If you are just using One Hot then INPUT_DIMENSION would be 4 because there are 4 possible unique nucleosides\n",
    "    # But if you are using K mers then INPUT_DIMENSION would be number of unique k-mers on all dataset\n",
    "\n",
    "    # LSTM model Format - (batch_size, sequence_length, input_size)  \n",
    "    # batch_size comes from DataLoaded , you can tune it.\n",
    "    # Sequence Length - Lenth of X[0] shape (32, 19, 12)  Here 19 is sequence length , \n",
    "    # Input Size = 12 length of X[0][0] - Basically all unique nucleoside or k-mers\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = RNAClassifier_BCE(input_dim=12, hidden_dim=HIDDEN_DIMENSION, output_dim=OUTPUT_DIMENSION)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Number of Parameters for Model\n",
    "    total_parameters = []\n",
    "    for p in model.parameters():\n",
    "        total_parameters.append(p.numel())\n",
    "    \n",
    "    print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        # Basically models predict probability for each class and then finally we pick up with hightest probabiliyt class \n",
    "        # and then compare with actual class to calculate accuracy. \n",
    "        \n",
    "        ## Predicted torch.max(outputs, 1) \n",
    "#         values=tensor([0.7304, 0.4554, 0.3984, 0.4091, 0.9271, 0.4370, 0.3837, 0.4395, 0.4005,\n",
    "#         0.5451, 0.5642, 0.6866, 0.5768, 0.5160, 0.5093, 0.4947, 0.6823, 0.4612,\n",
    "#         0.4073, 0.5041, 0.6982, 0.4116, 0.4092, 0.5589, 0.6010, 0.4693, 0.6324,\n",
    "#         0.5524, 0.4339, 0.7889, 0.5216, 0.6588]),\n",
    "#         indices=tensor([ 1, 10, 10, 10,  1, 10, 10, 10,  1, 10,  1,  1,  0,  1,  1, 10,  1, 10,\n",
    "#         10, 10, 10,  1, 10,  0, 10,  1,  1,  0, 10,  1,  1,  0]))\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class index with the highest probability\n",
    "        print(torch.max(outputs, 1))\n",
    "        true_labels = torch.max(y_batch, 1).indices  # Get the true class indices from the one-hot encoded labels [0,1,0,0,,,... ] Get index of 1\n",
    "        correct_predictions += (predicted == true_labels).sum().item()\n",
    "        total_predictions += y_batch.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA_ModX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
