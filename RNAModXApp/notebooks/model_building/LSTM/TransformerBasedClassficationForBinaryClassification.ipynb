{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkDJ1UJK69ON",
        "outputId": "77279665-08bc-4cee-85a2-527ed5204f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (4.3.1)\n",
            "Requirement already satisfied: imbalanced-learn in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (0.10.1)\n",
            "Requirement already satisfied: xgboost in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (1.7.5)\n",
            "Requirement already satisfied: torch in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (2.0.1)\n",
            "Collecting shap\n",
            "  Downloading shap-0.42.1-cp310-cp310-win_amd64.whl (462 kB)\n",
            "                                              0.0/462.3 kB ? eta -:--:--\n",
            "     ------------------------------------  460.8/462.3 kB 14.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- 462.3/462.3 kB 9.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (1.10.4)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (2.0.12)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from shap) (1.5.3)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Collecting numba (from shap)\n",
            "  Downloading numba-0.57.1-cp310-cp310-win_amd64.whl (2.5 MB)\n",
            "                                              0.0/2.5 MB ? eta -:--:--\n",
            "     --------------                           0.9/2.5 MB 28.7 MB/s eta 0:00:01\n",
            "     -------------------------------          2.0/2.5 MB 25.7 MB/s eta 0:00:01\n",
            "     ----------------------------------       2.2/2.5 MB 23.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.5/2.5 MB 14.7 MB/s eta 0:00:00\n",
            "Collecting cloudpickle (from shap)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: Mako in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from tqdm->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
            "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba->shap)\n",
            "  Downloading llvmlite-0.40.1-cp310-cp310-win_amd64.whl (27.7 MB)\n",
            "                                              0.0/27.7 MB ? eta -:--:--\n",
            "     -                                        0.9/27.7 MB 29.4 MB/s eta 0:00:01\n",
            "     ---                                      2.2/27.7 MB 28.4 MB/s eta 0:00:01\n",
            "     ----                                     3.1/27.7 MB 24.3 MB/s eta 0:00:02\n",
            "     -----                                    3.7/27.7 MB 21.4 MB/s eta 0:00:02\n",
            "     ------                                   4.8/27.7 MB 21.6 MB/s eta 0:00:02\n",
            "     --------                                 6.0/27.7 MB 22.6 MB/s eta 0:00:01\n",
            "     ---------                                6.9/27.7 MB 22.1 MB/s eta 0:00:01\n",
            "     -----------                              8.0/27.7 MB 23.2 MB/s eta 0:00:01\n",
            "     -------------                            9.0/27.7 MB 22.2 MB/s eta 0:00:01\n",
            "     --------------                          10.2/27.7 MB 22.4 MB/s eta 0:00:01\n",
            "     ---------------                         11.2/27.7 MB 21.8 MB/s eta 0:00:01\n",
            "     -----------------                       12.5/27.7 MB 21.8 MB/s eta 0:00:01\n",
            "     -------------------                     13.6/27.7 MB 23.4 MB/s eta 0:00:01\n",
            "     --------------------                    14.6/27.7 MB 23.4 MB/s eta 0:00:01\n",
            "     ----------------------                  15.8/27.7 MB 23.4 MB/s eta 0:00:01\n",
            "     -----------------------                 16.9/27.7 MB 23.4 MB/s eta 0:00:01\n",
            "     -------------------------               18.1/27.7 MB 23.4 MB/s eta 0:00:01\n",
            "     ---------------------------             19.2/27.7 MB 24.2 MB/s eta 0:00:01\n",
            "     ----------------------------            20.4/27.7 MB 24.2 MB/s eta 0:00:01\n",
            "     ------------------------------          21.7/27.7 MB 25.2 MB/s eta 0:00:01\n",
            "     --------------------------------        22.8/27.7 MB 25.2 MB/s eta 0:00:01\n",
            "     ---------------------------------       24.0/27.7 MB 24.2 MB/s eta 0:00:01\n",
            "     -----------------------------------     25.0/27.7 MB 25.2 MB/s eta 0:00:01\n",
            "     ------------------------------------    26.3/27.7 MB 25.1 MB/s eta 0:00:01\n",
            "     --------------------------------------  27.1/27.7 MB 26.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  27.7/27.7 MB 25.2 MB/s eta 0:00:01\n",
            "     --------------------------------------- 27.7/27.7 MB 21.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from pandas->shap) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
            "Successfully installed cloudpickle-2.2.1 llvmlite-0.40.1 numba-0.57.1 shap-0.42.1 slicer-0.0.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna  scikit-learn gensim imbalanced-learn xgboost torch shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ohWCGHSq5BVM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        " #Import All Libraries Here\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import time\n",
        "from collections import Counter\n",
        "# PyTorch Import\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load dataset and Encode with n-mer encoding techniques \n",
        "## train Model \n",
        "## test model \n",
        "## get summary report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQvMm5lH5BVO",
        "outputId": "c95da561-bad2-41a7-f349-7ee858113f35"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# BINARY_DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/Atol_with_ROS.csv\"\n",
        "#ENCODING_FILE = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/3-mer-dictionary.pkl\"\n",
        "\n",
        "# Record Constants\n",
        "# Record Constants\n",
        "\n",
        "BINARY_DATA_PATH = \"../../../data/hAm_with_ROS.csv\"\n",
        "ENCODING_FILE = \"3-mer-dictionary.pkl\"\n",
        "\n",
        "# INPUT_TRAIN_IN = \"../../../data/train_in.csv\"\n",
        "# INPUT_TRAIN_OUT = \"../../../data/train_out.csv\"\n",
        "# INPUT_TEST_IN = \"../../../data/test_in.csv\"\n",
        "# INPUT_TEST_OUT = \"../../../data/test_out.csv\"\n",
        "# INPUT_VALIDATION_IN = \"../../../data/valid_in_nucleo.csv\"\n",
        "# INPUT_VALIDATION_OUT  = \"../../../data/valid_out.csv\"\n",
        "\n",
        "# TARGET_MODEL_PATH = '../../webapp/model_files'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Class , Train and Valid Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "class RNATransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n",
        "        super(RNATransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        # If batch size first is true then it should be batch size , sequence lenght , embedding dimension\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=hidden_dim , batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        #print(\"Shape of Original X  \", x.shape)\n",
        "        x_embedded = self.embedding(x)\n",
        "        #print(\"Shape of X embedded\" , x_embedded.shape)\n",
        "        x_transformed = self.transformer_encoder(x_embedded)\n",
        "        #print(\"Shape of Transformed X\" , x_transformed.shape)\n",
        "        x_transformed = x_transformed[:, -1, :]  # taking the last token's output\n",
        "\n",
        "        output = self.dropout(x_transformed)\n",
        "        out = self.fc(output)\n",
        "        return out.squeeze()\n",
        "\n",
        "\n",
        "def validate_model(model, test_dataloader , device ,loss_function):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model(inputs)\n",
        "            if outputs.size() != labels.size(): # skip if batch size mismatch\n",
        "              continue\n",
        "            #print(f\"Shape of Label : {labels.shape} and output shape : {outputs.shape} \")\n",
        "            loss = loss_function(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float() # Set threshold at 0.5\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())  # Capture True Labels for Summary Report\n",
        "            predicted_labels.extend(predicted.cpu().numpy()) # Capture Predicted Labels Lables for Summary Report\n",
        "\n",
        "    validation_loss = running_loss / len(test_dataloader)\n",
        "    validation_accuracy = correct / total\n",
        "\n",
        "    return validation_loss , validation_accuracy , true_labels , predicted_labels\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, device, epochs, optimizer, loss_function):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        #progress_bar = tqdm(train_dataloader, desc='Epoch {:03d}'.format(epoch + 1), leave=False, disable=False)\n",
        "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            if outputs.size() != labels.size(): # skip if batch size mismatch\n",
        "              continue\n",
        "            #print(f\"Shape of Label : {labels.shape} and output shape : {outputs.shape} \")\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            #progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(inputs))})\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        val_loss,  validation_accuracy , true_labels , predicted_labels = validate_model(model, test_dataloader, device, loss_function)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Test Accuracy: {validation_accuracy:.4f} , Time Taken : {elapsed_time}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count == 10:\n",
        "                print(\"No improvement in validation loss for 5 epochs. Training stopped.\")\n",
        "                break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcUOufcj5BVO",
        "outputId": "41fceabe-be67-4d6b-dff9-09bb9427a285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Dataset  : (309214, 102)\n"
          ]
        }
      ],
      "source": [
        "#Read X Varaibles and Y Varaibles\n",
        "\n",
        "binary_raw_dataframe =  pd.read_csv(BINARY_DATA_PATH, header=None  , skiprows=1)\n",
        "print(f\"Shape of Dataset  : {binary_raw_dataframe.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KEU0FAuaLDKm",
        "outputId": "c5f95ee6-2d45-4ad9-9ac2-5e0395114e9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309209</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309210</th>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309211</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309212</th>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309213</th>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>hAm</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>309214 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0   1   2   3   4   5   6   7   8   9    ... 92  93  94  95  96  97   \\\n",
              "0        T   T   G   C   C   A   C   A   C   T  ...   C   A   G   T   A   T   \n",
              "1        T   T   T   G   A   A   A   A   A   A  ...   T   C   A   T   C   G   \n",
              "2        A   G   A   A   A   C   A   T   T   C  ...   T   T   C   T   G   T   \n",
              "3        T   T   A   G   T   T   T   T   A   C  ...   A   A   A   A   A   T   \n",
              "4        C   A   A   C   A   G   A   A   G   T  ...   A   A   A   A   T   G   \n",
              "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
              "309209   A   A   C   A   C   T   A   T   A   C  ...   G   T   T   T   T   A   \n",
              "309210   A   T   T   C   A   G   T   C   C   T  ...   A   C   C   T   G   A   \n",
              "309211   C   T   T   G   A   G   T   C   G   T  ...   A   A   G   T   T   A   \n",
              "309212   G   T   T   A   A   T   G   G   A   G  ...   A   T   A   A   C   T   \n",
              "309213   G   T   A   T   A   C   T   C   C   T  ...   C   A   G   A   A   G   \n",
              "\n",
              "       98  99  100  101  \n",
              "0        C   T   C  hAm  \n",
              "1        T   G   C  hAm  \n",
              "2        T   C   A  hAm  \n",
              "3        T   T   C  hAm  \n",
              "4        T   A   C  hAm  \n",
              "...     ..  ..  ..  ...  \n",
              "309209   A   A   C  hAm  \n",
              "309210   A   G   G  hAm  \n",
              "309211   A   C   C  hAm  \n",
              "309212   C   A   G  hAm  \n",
              "309213   T   C   T  hAm  \n",
              "\n",
              "[309214 rows x 102 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "binary_raw_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwZ2AFCo5BVO"
      },
      "source": [
        "### Calculate Sequence Positions to extracted from Original Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uWqxeK7J5BVP"
      },
      "outputs": [],
      "source": [
        "x_data = binary_raw_dataframe.iloc[:,0:101] # Select data\n",
        "y_data = binary_raw_dataframe.iloc[:,101] # Select Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jRDnwbiM5BVP",
        "outputId": "037e8a0f-a716-42f4-da59-b99e7b4d7c70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309209</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309210</th>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309211</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309212</th>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309213</th>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>309214 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0   1   2   3   4   5   6   7   8   9    ... 91  92  93  94  95  96   \\\n",
              "0        T   T   G   C   C   A   C   A   C   T  ...   G   C   A   G   T   A   \n",
              "1        T   T   T   G   A   A   A   A   A   A  ...   C   T   C   A   T   C   \n",
              "2        A   G   A   A   A   C   A   T   T   C  ...   T   T   T   C   T   G   \n",
              "3        T   T   A   G   T   T   T   T   A   C  ...   G   A   A   A   A   A   \n",
              "4        C   A   A   C   A   G   A   A   G   T  ...   T   A   A   A   A   T   \n",
              "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
              "309209   A   A   C   A   C   T   A   T   A   C  ...   A   G   T   T   T   T   \n",
              "309210   A   T   T   C   A   G   T   C   C   T  ...   T   A   C   C   T   G   \n",
              "309211   C   T   T   G   A   G   T   C   G   T  ...   C   A   A   G   T   T   \n",
              "309212   G   T   T   A   A   T   G   G   A   G  ...   G   A   T   A   A   C   \n",
              "309213   G   T   A   T   A   C   T   C   C   T  ...   T   C   A   G   A   A   \n",
              "\n",
              "       97  98  99  100  \n",
              "0        T   C   T   C  \n",
              "1        G   T   G   C  \n",
              "2        T   T   C   A  \n",
              "3        T   T   T   C  \n",
              "4        G   T   A   C  \n",
              "...     ..  ..  ..  ..  \n",
              "309209   A   A   A   C  \n",
              "309210   A   A   G   G  \n",
              "309211   A   A   C   C  \n",
              "309212   T   C   A   G  \n",
              "309213   G   T   C   T  \n",
              "\n",
              "[309214 rows x 101 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om8FAJ0rLDKo",
        "outputId": "1eae1d3b-28c1-4533-bec7-ed324a399249"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         hAm\n",
              "1         hAm\n",
              "2         hAm\n",
              "3         hAm\n",
              "4         hAm\n",
              "         ... \n",
              "309209    hAm\n",
              "309210    hAm\n",
              "309211    hAm\n",
              "309212    hAm\n",
              "309213    hAm\n",
              "Name: 101, Length: 309214, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Fixed Numerical Encoding From File For Model Interpretability "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KLuiCU2X5BVP"
      },
      "outputs": [],
      "source": [
        "\n",
        "concatenated_column= x_data.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
        "x_data_with_complete_sequence = x_data.assign(Sequence=concatenated_column)\n",
        "\n",
        "\n",
        "with open(ENCODING_FILE, 'rb') as f:\n",
        "    kmer_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "LvSkKP5vLDKp",
        "outputId": "6f4590a3-2a5f-4ef3-d6c9-764e5f92c241"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCAT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>TTTGAAAAAATATTAGCAATGTGAGGACACTTAAGCAGTTTTGTCA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>AGAAACATTCAACCTCCCTTCTTTTTATTCCAGTTGTCCTTTTCTC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>TTAGTTTTACTATGGAATCATAATAACCCACATAGAAGACTGATAT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>CAACAGAAGTTTCTCATCTATAATCAGTAGCACTAAACTCTTGGTT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309209</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>AACACTATACACTTAAGGCTACACTAAATTCATTTTTTAAAATTTT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309210</th>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>ATTCAGTCCTCTGAATTAAGATATTAGGTTATAAGGCCATGTACAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309211</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>CTTGAGTCGTGATCACACCACTGTACTCCAGCTTGTCTCCAAATAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309212</th>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>GTTAATGGAGAAGACATATACATTACTTGAATAATTTAAGTCTGAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309213</th>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>GTATACTCCTTTCTCTTCACACACTTTCATGGCAACATGCTGTTGA...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>309214 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0  1  2  3  4  5  6  7  8  9  ... 92 93 94 95 96 97 98 99 100  \\\n",
              "0       T  T  G  C  C  A  C  A  C  T  ...  C  A  G  T  A  T  C  T   C   \n",
              "1       T  T  T  G  A  A  A  A  A  A  ...  T  C  A  T  C  G  T  G   C   \n",
              "2       A  G  A  A  A  C  A  T  T  C  ...  T  T  C  T  G  T  T  C   A   \n",
              "3       T  T  A  G  T  T  T  T  A  C  ...  A  A  A  A  A  T  T  T   C   \n",
              "4       C  A  A  C  A  G  A  A  G  T  ...  A  A  A  A  T  G  T  A   C   \n",
              "...    .. .. .. .. .. .. .. .. .. ..  ... .. .. .. .. .. .. .. ..  ..   \n",
              "309209  A  A  C  A  C  T  A  T  A  C  ...  G  T  T  T  T  A  A  A   C   \n",
              "309210  A  T  T  C  A  G  T  C  C  T  ...  A  C  C  T  G  A  A  G   G   \n",
              "309211  C  T  T  G  A  G  T  C  G  T  ...  A  A  G  T  T  A  A  C   C   \n",
              "309212  G  T  T  A  A  T  G  G  A  G  ...  A  T  A  A  C  T  C  A   G   \n",
              "309213  G  T  A  T  A  C  T  C  C  T  ...  C  A  G  A  A  G  T  C   T   \n",
              "\n",
              "                                                 Sequence  \n",
              "0       TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCAT...  \n",
              "1       TTTGAAAAAATATTAGCAATGTGAGGACACTTAAGCAGTTTTGTCA...  \n",
              "2       AGAAACATTCAACCTCCCTTCTTTTTATTCCAGTTGTCCTTTTCTC...  \n",
              "3       TTAGTTTTACTATGGAATCATAATAACCCACATAGAAGACTGATAT...  \n",
              "4       CAACAGAAGTTTCTCATCTATAATCAGTAGCACTAAACTCTTGGTT...  \n",
              "...                                                   ...  \n",
              "309209  AACACTATACACTTAAGGCTACACTAAATTCATTTTTTAAAATTTT...  \n",
              "309210  ATTCAGTCCTCTGAATTAAGATATTAGGTTATAAGGCCATGTACAA...  \n",
              "309211  CTTGAGTCGTGATCACACCACTGTACTCCAGCTTGTCTCCAAATAA...  \n",
              "309212  GTTAATGGAGAAGACATATACATTACTTGAATAATTTAAGTCTGAA...  \n",
              "309213  GTATACTCCTTTCTCTTCACACACTTTCATGGCAACATGCTGTTGA...  \n",
              "\n",
              "[309214 rows x 102 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data_with_complete_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk_WaSe35BVP"
      },
      "source": [
        "### 3 mer coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JwIzW5gY5BVQ"
      },
      "outputs": [],
      "source": [
        "kmer_dict = {}\n",
        "encoding = 0\n",
        "k = 3\n",
        "\n",
        "# for sequence in x_data_with_complete_sequence['Sequence']:\n",
        "#     for i in range(len(sequence) - k + 1):\n",
        "#         kmer = sequence[i:i+k]\n",
        "#         if kmer not in kmer_dict:\n",
        "#             kmer_dict[kmer] = encoding\n",
        "#             encoding += 1\n",
        "\n",
        "\n",
        "\n",
        "number_of_unique_kmers = set()\n",
        "def encode_seq(kmer_token):\n",
        "\n",
        "    # A 1 0 0 0\n",
        "    # C 0 1 0 0\n",
        "    # T/U 0 0 0 1\n",
        "    # G 0 0 1 0\n",
        "    # N 0 0 0 0\n",
        "\n",
        "    encoding_dict = {\n",
        "        'A': [1, 0, 0, 0],\n",
        "        'C': [0, 1, 0, 0],\n",
        "        'G': [0, 0, 1, 0],\n",
        "        'T': [0, 0, 0, 1],\n",
        "        'U': [0, 0, 0, 1],\n",
        "        'N': [0, 0, 0, 0],\n",
        "    }\n",
        "\n",
        "    encoded_sequence = []\n",
        "    number_of_unique_kmers.add(kmer_token)\n",
        "    for  base in kmer_token:\n",
        "        encoded_sequence.append(encoding_dict[base])\n",
        "    return np.array(encoded_sequence).flatten()\n",
        "\n",
        "def applyOneHotEncoding(tokenized_sequences):\n",
        "    encoded_sequences = []\n",
        "    for seq in tokenized_sequences:\n",
        "        encoded_sequences.append(encode_seq(seq)) \n",
        " \n",
        "    return np.array(encoded_sequences).flatten()\n",
        "\n",
        "\n",
        "def encode_with_k_mer_codon(sequence):\n",
        "    encoded_sequence = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        encoded_sequence.append(kmer_dict[sequence[i:i+k]] )\n",
        "\n",
        "    return np.array(encoded_sequence)\n",
        "\n",
        "def convertToTensor(x):\n",
        "    return torch.tensor(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWMPH3TYLDKp"
      },
      "source": [
        "### Encode X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ULG0B0e95BVQ"
      },
      "outputs": [],
      "source": [
        "X_encoded  =  x_data_with_complete_sequence['Sequence'].apply(encode_with_k_mer_codon)\n",
        "X_encoded = torch.tensor(X_encoded , dtype =  torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne-hmuDd5BVR",
        "outputId": "600e066c-ae51-428f-f37f-b853e9ae152b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([309214, 99])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0rGCEUqLDKq"
      },
      "source": [
        "### Encode Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMBhQIW9LDKq",
        "outputId": "ce915cb7-889b-4d1d-e6d2-049abaad32fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Values :  hAm       154607\n",
            "NonMoD    154607\n",
            "Name: 101, dtype: int64\n",
            "{0: 154607, 1: 154607}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# encoder = LabelEncoder()\n",
        "# y_data = pd.Series(y_data.squeeze())\n",
        "# y_encoded = encoder.fit_transform(y_data)\n",
        "\n",
        "print(\"Unique Values : \" , y_data.value_counts())\n",
        "y_data = pd.Series(y_data.squeeze())\n",
        "y_encoded = y_data.where(y_data=='NonMoD', other=1).replace('NonMoD', 0)\n",
        "\n",
        "\n",
        "unique, counts = np.unique(y_encoded, return_counts=True)\n",
        "value_counts = dict(zip(unique, counts))\n",
        "print(value_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(309214,)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_encoded  = np.array(y_encoded)\n",
        "y_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etAEiUuL5BVR",
        "outputId": "fc019ad1-06f0-4e5c-eadb-c142c1dc4b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate Train and Split..\n"
          ]
        }
      ],
      "source": [
        "print(\"Generate Train and Split..\")\n",
        "# Train set\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test and Validation set\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxuA3OdC5BVR",
        "outputId": "1fbbb675-c0e8-4c70-acae-13ca2e509a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Y Count :  Counter({1: 108282, 0: 108167})\n",
            "Test Y Count :  Counter({1: 23252, 0: 23131})\n"
          ]
        }
      ],
      "source": [
        "X_encoded = None\n",
        "y_encoded = None\n",
        "x_data_filtered , y_data_filtered = None,None\n",
        "x_data , y_data = None , None\n",
        "\n",
        "x_data_with_complete_sequence = None\n",
        "\n",
        "print(\"Train Y Count : \" ,Counter(y_train))\n",
        "print(\"Test Y Count : \" ,Counter(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zb-kIy35BVR"
      },
      "source": [
        "### Balance Datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3tEqKJG25BVR"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(y_train , dtype=torch.long)\n",
        "y_test = torch.tensor(y_test , dtype=torch.long)\n",
        "y_valid = torch.tensor(y_valid , dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpftneeP5BVR",
        "outputId": "955ad4b3-45d3-46b2-9070-714c75955279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([216449, 99])\n",
            "torch.Size([216449])\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM0C9uYE5BVS",
        "outputId": "bcfc9b80-aa6d-48d1-ac88-7b19e77f2103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X Train : torch.Size([216449, 99])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of X Train : {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pIOzPJWY5BVS"
      },
      "outputs": [],
      "source": [
        "hyperparameter = {}\n",
        "hyperparameter['INPUT_DIMENSION'] = len(kmer_dict) # For One Hot Encoding Input Dimension would be 4 as there only 4 unique nucleocide\n",
        "hyperparameter['HIDDEN_DIMENSION'] = 32\n",
        "hyperparameter['NO_OF_LAYERS'] = 4\n",
        "hyperparameter['BATCH_SIZE'] = 32\n",
        "hyperparameter['OUTPUT_DIMENSION'] = 1\n",
        "hyperparameter['EMBEDDING_DIMENSION'] = 256 # if you are using Word2Vec Encoding then this should be same as Word2Vec Embedding Dim\n",
        "hyperparameter['DROP_OUT'] = 0.4\n",
        "hyperparameter['LEARNING_RATE'] = 0.00001\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = RNADataset(X_train, y_train)\n",
        "test_dataset = RNADataset(X_test, y_test)\n",
        "valid_dataset = RNADataset(X_valid, y_valid)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sXVdBroi5BVS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sP484k65BVS",
        "outputId": "3c743ef4-c014-4a17-d9f6-67eed46f47cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 99])\n"
          ]
        }
      ],
      "source": [
        "# Check data is in correct shape - batch size , sequece len , embedding dimension size\n",
        "for inputs, labels in train_dataloader:\n",
        "    print(inputs.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7wtvOzF5BVS",
        "outputId": "02e5213a-059f-416e-f5c0-6d4f5865e23c"
      },
      "outputs": [],
      "source": [
        "model = RNATransformerModel(input_dim=hyperparameter['INPUT_DIMENSION'],\n",
        "                            embedding_dim=hyperparameter['EMBEDDING_DIMENSION'],\n",
        "                            hidden_dim=hyperparameter['HIDDEN_DIMENSION'] ,\n",
        "                            num_layers = hyperparameter['NO_OF_LAYERS'],\n",
        "                            output_dim=hyperparameter['OUTPUT_DIMENSION'],\n",
        "                            dropout=hyperparameter['DROP_OUT'] )\n",
        "\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()  ## MSELoss of Regression problem  # BCELoss for binary classification\n",
        "optimizer = optim.Adam(model.parameters() ,  lr=hyperparameter['LEARNING_RATE'])\n",
        "\n",
        "# Number of Parameters for Model\n",
        "total_parameters = []\n",
        "for p in model.parameters():\n",
        "    total_parameters.append(p.numel())\n",
        "\n",
        "print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model Parameters  : \" , hyperparameter)\n",
        "\n",
        "# Train Model with configured Parameter\n",
        "train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZNbHvgEhxt0",
        "outputId": "cf65a9c7-1b08-441f-afae-ee328fd523e5"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8rs97X-5BVS",
        "outputId": "72807393-14ac-4ae1-a027-54d93d074c43"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "_, final_accuracy, true_labels, predicted_labels = validate_model(model, valid_dataloader,device,loss_function)\n",
        "\n",
        "# Print the final accuracy\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Print the classification summary\n",
        "print(\"\\n Classification Summary:\")\n",
        "print(classification_report(true_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automate Train and Test for All Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 1\n",
        "\n",
        "if k != 1 :\n",
        "    with open(ENCODING_FILE, 'rb') as f:\n",
        "        kmer_dict = pickle.load(f)\n",
        "\n",
        "hyperparameter = {}\n",
        "hyperparameter['INPUT_DIMENSION'] = len(kmer_dict) # For One Hot Encoding Input Dimension would be 4 as there only 4 unique nucleocide\n",
        "hyperparameter['HIDDEN_DIMENSION'] = 32\n",
        "hyperparameter['NO_OF_LAYERS'] = 4\n",
        "hyperparameter['BATCH_SIZE'] = 32\n",
        "hyperparameter['OUTPUT_DIMENSION'] = 1\n",
        "hyperparameter['EMBEDDING_DIMENSION'] = 256 # if you are using Word2Vec Encoding then this should be same as Word2Vec Embedding Dim\n",
        "hyperparameter['DROP_OUT'] = 0.4\n",
        "hyperparameter['LEARNING_RATE'] = 0.00001\n",
        "\n",
        "\n",
        "number_of_unique_kmers = set()\n",
        "def encode_seq(kmer_token):\n",
        "\n",
        "    # A 1 0 0 0\n",
        "    # C 0 1 0 0\n",
        "    # T/U 0 0 0 1\n",
        "    # G 0 0 1 0\n",
        "    # N 0 0 0 0\n",
        "\n",
        "    encoding_dict = {\n",
        "        'A': [1, 0, 0, 0],\n",
        "        'C': [0, 1, 0, 0],\n",
        "        'G': [0, 0, 1, 0],\n",
        "        'T': [0, 0, 0, 1],\n",
        "        'U': [0, 0, 0, 1],\n",
        "        'N': [0, 0, 0, 0],\n",
        "    }\n",
        "\n",
        "    encoded_sequence = []\n",
        "    number_of_unique_kmers.add(kmer_token)\n",
        "    for  base in kmer_token:\n",
        "        encoded_sequence.append(encoding_dict[base])\n",
        "    return np.array(encoded_sequence).flatten()\n",
        "\n",
        "def applyOneHotEncoding(tokenized_sequences):\n",
        "    encoded_sequences = []\n",
        "    for seq in tokenized_sequences:\n",
        "        encoded_sequences.append(encode_seq(seq)) \n",
        " \n",
        "    return np.array(encoded_sequences).flatten()\n",
        "\n",
        "def encode_with_k_mer_codon(sequence):\n",
        "    encoded_sequence = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        encoded_sequence.append(kmer_dict[sequence[i:i+k]] )\n",
        "\n",
        "    return np.array(encoded_sequence)\n",
        "\n",
        "def convertToTensor(x):\n",
        "    return torch.tensor(x)\n",
        "\n",
        "\n",
        "def prepare_dataset(x_data , y_data):\n",
        "\n",
        "    concatenated_column= x_data.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
        "    x_data_with_complete_sequence = x_data.assign(Sequence=concatenated_column)\n",
        "\n",
        "    X_encoded  =  x_data_with_complete_sequence['Sequence'].apply(encode_with_k_mer_codon)\n",
        "    X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n",
        "\n",
        "\n",
        "    print(\"Unique Values : \" , y_data.value_counts())\n",
        "    y_data = pd.Series(y_data.squeeze())\n",
        "    y_encoded = y_data.where(y_data=='NonMoD', other=1).replace('NonMoD', 0)\n",
        "    y_encoded  = np.array(y_encoded)\n",
        "\n",
        "    print(\"Generate Train and Split..\")\n",
        "    # Train set\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Test and Validation set\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "    X_encoded = None\n",
        "    y_encoded = None\n",
        "    x_data_filtered , y_data_filtered = None,None\n",
        "    x_data , y_data = None , None\n",
        "\n",
        "    x_data_with_complete_sequence = None\n",
        "\n",
        "    #print(\"Train Y Count : \" ,Counter(y_train))\n",
        "    #print(\"Test Y Count : \" ,Counter(y_test))\n",
        "\n",
        "\n",
        "    y_train = torch.tensor(y_train , dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test , dtype=torch.long)\n",
        "    y_valid = torch.tensor(y_valid , dtype=torch.long)\n",
        "\n",
        "    train_dataset = RNADataset(X_train, y_train)\n",
        "    test_dataset = RNADataset(X_test, y_test)\n",
        "    valid_dataset = RNADataset(X_valid, y_valid)\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle = True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "\n",
        "\n",
        "    return train_dataloader , test_dataloader ,  valid_dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(train_dataloader ,test_dataloader):\n",
        "    model = RNATransformerModel(input_dim=hyperparameter['INPUT_DIMENSION'],\n",
        "                            embedding_dim=hyperparameter['EMBEDDING_DIMENSION'],\n",
        "                            hidden_dim=hyperparameter['HIDDEN_DIMENSION'] ,\n",
        "                            num_layers = hyperparameter['NO_OF_LAYERS'],\n",
        "                            output_dim=hyperparameter['OUTPUT_DIMENSION'],\n",
        "                            dropout=hyperparameter['DROP_OUT'] )\n",
        "\n",
        "\n",
        "    loss_function = nn.BCEWithLogitsLoss()  ## MSELoss of Regression problem  # BCELoss for binary classification\n",
        "    optimizer = optim.Adam(model.parameters() ,  lr=hyperparameter['LEARNING_RATE'])\n",
        "\n",
        "    # Number of Parameters for Model\n",
        "    total_parameters = []\n",
        "    for p in model.parameters():\n",
        "        total_parameters.append(p.numel())\n",
        "\n",
        "    print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
        "\n",
        "    # Train the model\n",
        "    num_epochs = 20\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    print(\"Model Parameters  : \" , hyperparameter)\n",
        "\n",
        "    # Train Model with configured Parameter\n",
        "    train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)\n",
        "\n",
        "    return model , loss_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_accuracy(valid_dataloader , model , loss_function):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Evaluate the model on the test dataset\n",
        "    _, final_accuracy, true_labels, predicted_labels = validate_model(model, valid_dataloader,device,loss_function)\n",
        "\n",
        "    # Print the final accuracy\n",
        "    print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "    # Print the classification summary\n",
        "    print(\"\\n Classification Summary:\")\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "\n",
        "    return final_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BINARY_DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/Atol_with_ROS.csv\"\n",
        "#ENCODING_FILE = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/3-mer-dictionary.pkl\"\n",
        "\n",
        "# 'hAm', 'hCm', 'hGm','hTm','hm1A', 'hm5C', 'hm5U',\n",
        "\n",
        "class_list = ['hm6A','hm6Am','hm7G','hPsi','Atol']\n",
        "class_accuracy_dict = {}\n",
        "\n",
        "for mclass in class_list:\n",
        "\n",
        "    BINARY_DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/\"+mclass+\"_with_ROS.csv\"\n",
        "    #ENCODING_FILE = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/3-mer-dictionary.pkl\"\n",
        "\n",
        "    #BINARY_DATA_PATH = \"../../../data/\"+mclass+\"_with_ROS.csv\"\n",
        "    print(\"Processing File : \" , BINARY_DATA_PATH)\n",
        "    binary_raw_dataframe =  pd.read_csv(BINARY_DATA_PATH, header=None  , skiprows=1)\n",
        "    print(f\"Shape of Dataset  : {binary_raw_dataframe.shape}\")\n",
        "\n",
        "    x_data = binary_raw_dataframe.iloc[:,0:101] # Select data\n",
        "    y_data = binary_raw_dataframe.iloc[:,101] # Select Target\n",
        "\n",
        "    train_dataloader , test_dataloader ,  valid_dataloader = prepare_dataset(x_data,y_data)\n",
        "\n",
        "    model , loss_function  = train(train_dataloader, test_dataloader)\n",
        "\n",
        "    print(\"Accuracy for Class \" , mclass)\n",
        "    final_accuracy  = calculate_accuracy(valid_dataloader , model , loss_function)\n",
        "\n",
        "    # Nullify Object\n",
        "    x_data = None\n",
        "    y_data = None\n",
        "    binary_raw_dataframe = None\n",
        "\n",
        "    class_accuracy_dict[mclass] = final_accuracy\n",
        "\n",
        "    model_path =  '/content/drive/My Drive/Colab Notebooks/Capstone Project/model/Binary/'+mclass+'_model.pt'\n",
        "    torch.save(model, model_path)\n",
        "\n",
        "    break # Reduce Unnecessary GPU usage\n",
        "\n",
        "print(\"Final Result : \" , class_accuracy_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pre Trained Model and Perform Model Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNATransformerModel(\n",
              "  (embedding): Embedding(94, 256)\n",
              "  (encoder_layer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=256, out_features=32, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=32, out_features=256, bias=True)\n",
              "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=32, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=32, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class RNATransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n",
        "        super(RNATransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        # If batch size first is true then it should be batch size , sequence lenght , embedding dimension\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=hidden_dim , batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        #print(\"Shape of Original X  \", x.shape)\n",
        "        x_embedded = self.embedding(x)\n",
        "        #print(\"Shape of X embedded\" , x_embedded.shape)\n",
        "        x_transformed = self.transformer_encoder(x_embedded)\n",
        "        #print(\"Shape of Transformed X\" , x_transformed.shape)\n",
        "        x_transformed = x_transformed[:, -1, :]  # taking the last token's output\n",
        "\n",
        "        output = self.dropout(x_transformed)\n",
        "        out = self.fc(output)\n",
        "        return out.squeeze()\n",
        "\n",
        "model_path =  \"../../../models/Transfomer_3Mer/hAm_model.pt\"\n",
        "model = torch.load(model_path ,map_location=torch.device('cpu'))\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding file successfully loaded.\n",
            "tensor([[35, 28, 29,  1, 36, 37, 36, 58, 25, 28, 24, 25, 45, 51, 57, 41, 30, 29,\n",
            "          6, 25, 28, 18, 60, 48, 20, 17, 29,  1, 60, 48, 20, 40, 21, 46,  4, 41,\n",
            "         16, 51, 39, 20, 21, 10, 11,  2, 12, 11,  2, 12, 11,  2, 12, 11, 36,  5,\n",
            "          1, 36,  5, 23, 38, 55,  3,  4,  5,  6, 14, 15, 30, 29,  0,  6, 14, 11,\n",
            "         19, 20, 17, 18, 36,  5,  1, 60, 48, 56, 53, 48, 56, 57,  5,  0, 23, 16,\n",
            "         17, 18, 19,  9, 46, 32, 12, 22, 14]])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Helper Function for Feature Encoding.\n",
        "'''\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def encode_with_k_mer_codon(sequence, kmer_dict, k):\n",
        "    encoded_sequence = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        encoded_sequence.append(kmer_dict[sequence[i:i + k]])\n",
        "    return np.array(encoded_sequence)\n",
        "\n",
        "\n",
        "def encode_sequence(sequence: str, encoding_file: str):\n",
        "    k = 3\n",
        "    kmer_dict = {}\n",
        "    try:\n",
        "        with open(encoding_file, 'rb') as f:\n",
        "            kmer_dict = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        raise ValueError(\"File not found! Please ensure the file path is correct.\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"An error occurred while loading the file: \" + str(e))\n",
        "\n",
        "    print(f\"Encoding file successfully loaded.\")\n",
        "\n",
        "    if len(sequence) != 101:\n",
        "        raise ValueError('Invalid Sequence Length. Expected Sequence Length is 101.')\n",
        "\n",
        "    x_encoded = encode_with_k_mer_codon(sequence, kmer_dict, k)\n",
        "    X_encoded = torch.tensor([x_encoded], dtype=torch.long)\n",
        "\n",
        "    return X_encoded # Adding extra dimension for batch size \n",
        "\n",
        "\n",
        "\n",
        "encoding_file = 'C:/Users/shashi.vish/Documents/Shashi/Education/HigherEducation/NUS/Capstone Project/Git/RNA-ModX/RNAModXApp//notebooks/model_building/LSTM//3-mer-dictionary.pkl'\n",
        "sequence = 'TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCATCATCATCACCACCGATACCTCGCCCTCAGGCACCAAGAAGACCCGGCAGTATCTC'\n",
        "x_train = encode_sequence(sequence, encoding_file)\n",
        "print(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train  torch.Size([1, 99])\n",
            "tensor([6.1201])\n",
            "Raw Output :  tensor(6.1201)  Shape :  torch.Size([])\n",
            "Probabilities :  tensor(0.9978)\n",
            "Predicted Class :  tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "# Make Predictoin\n",
        "\n",
        "# 0  - Non Modified RNA Nucleoside\n",
        "# 1  - Corresponding Modified Nucleoside\n",
        "\n",
        "print(\"Shape of X_train \" , x_train.shape)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(x_train)\n",
        "    print(output.unsqueeze(-1))\n",
        "    print(\"Raw Output : \" , output ,\" Shape : \" , output.shape )\n",
        "    probabilities = torch.sigmoid(output)\n",
        "    print(\"Probabilities : \" , probabilities)\n",
        "    predicted_class = (probabilities > 0.5).float()  \n",
        "    print(\"Predicted Class : \" , predicted_class)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mWrapperModel\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[0;32m      3\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class WrapperModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).unsqueeze(-1)\n",
        "        \n",
        "wrapped_model = WrapperModel(model)\n",
        "\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()\n",
        "\n",
        "\n",
        "\n",
        "# we use the first 100 training examples as our background dataset to integrate over\n",
        "explainer = shap.DeepExplainer(wrapped_model,  X_train[:1000])\n",
        "\n",
        "# explain the first 10 predictions\n",
        "# explaining each prediction requires 2 * background dataset size runs\n",
        "shap_values = explainer.shap_values(x_train)\n",
        "\n",
        "print(shap_values)\n",
        "\n",
        "# # plot the feature attributions\n",
        "shap.summary_plot(shap_values, x_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask\n",
            "  Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\n",
            "                                              0.0/96.9 kB ? eta -:--:--\n",
            "     ----------------                         41.0/96.9 kB 2.0 MB/s eta 0:00:01\n",
            "     ------------------------------------   92.2/96.9 kB 880.9 kB/s eta 0:00:01\n",
            "     -------------------------------------- 96.9/96.9 kB 794.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: Werkzeug>=2.3.3 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from flask) (2.3.6)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from flask) (3.1.2)\n",
            "Collecting itsdangerous>=2.1.2 (from flask)\n",
            "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click>=8.1.3 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from flask) (8.1.3)\n",
            "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from flask) (1.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shashi.vish\\python environment\\rna_modx\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.2)\n",
            "Installing collected packages: itsdangerous, flask\n",
            "Successfully installed flask-2.3.2 itsdangerous-2.1.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install flask "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c230947687a8f3ffad2ce5baec6aac89e01c839661a42aacfb7c80a5442a49ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
