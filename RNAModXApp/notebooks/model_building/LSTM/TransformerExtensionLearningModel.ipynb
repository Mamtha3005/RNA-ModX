{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software engineer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Example question and context\n",
    "question = \"Who is Shashi?\"\n",
    "context = \"Shashi is software engineer.\"\n",
    "\n",
    "# Encode the inputs \n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "# Get the predicted answer\n",
    "output = model(**inputs)\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the score\n",
    "answer_start = torch.argmax(output.start_logits)  \n",
    "answer_end = torch.argmax(output.end_logits) + 1  \n",
    "\n",
    "# Get the answer\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "# Print the answer\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\n",
    "        \"context\": \"Shashi is software engineer.\",\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"id\": \"00001\",\n",
    "                \"question\": \"Who is Shashi?\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": \"software engineer\",\n",
    "                        \"answer_start\": 12\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:35<00:00, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 35.4116, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.085, 'train_loss': 12.04428482055664, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=12.04428482055664, metrics={'train_runtime': 35.4116, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.085, 'train_loss': 12.04428482055664, 'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# You have a list of dictionaries, so let's convert it to a dictionary of lists\n",
    "train_data_lists = {\n",
    "    \"context\": [\"Shashi is software engineer.\"],\n",
    "    \"question\": [\"Who is Shashi?\"],\n",
    "    \"answers\": [[{\"text\": \"software engineer\", \"answer_start\": 12}]]\n",
    "}\n",
    "\n",
    "valid_data_lists = {\n",
    "    \"context\": [\"Shashi lives in New York\"],\n",
    "    \"question\": [\"Where does he lives\"],\n",
    "    \"answers\": [[{\"text\": \"New York\", \"answer_start\": 14}]]\n",
    "}\n",
    "\n",
    "# Convert train_data to a Dataset object\n",
    "train_dataset = Dataset.from_dict(train_data_lists)\n",
    "valid_dataset = Dataset.from_dict(valid_data_lists)\n",
    "\n",
    "# Define a function to tokenize our examples\n",
    "def tokenize_examples(example):\n",
    "    # Encode our concatenated data\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        example[\"question\"], \n",
    "        example[\"context\"],\n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    # This is the tricky part, 'answer_start' is a character position. We need to find which token this position belongs to\n",
    "    # We can do that with the built in char_to_token method in the tokenizer\n",
    "    start_position = tokenizer(example['context'], return_offsets_mapping=True, truncation=True, max_length=512).char_to_token(example['answers'][0]['answer_start'])\n",
    "    end_position = start_position + len(example['answers'][0]['text'].split()) - 1\n",
    "\n",
    "    if start_position is None or end_position is None:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "\n",
    "    encoded.update({'start_positions': start_position, 'end_positions': end_position})\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Apply the function to our train_dataset\n",
    "train_dataset = train_dataset.map(tokenize_examples)\n",
    "valid_dataset = valid_dataset.map(tokenize_examples)\n",
    "\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (BertForQuestionAnswering) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mWho are you?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m output\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\transformers\\generation\\utils.py:1246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         synced_gpus \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[39m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 1246\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_class()\n\u001b[0;32m   1248\u001b[0m \u001b[39m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m generation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# legacy: users may modify the model configuration to control generation -- update the generation config\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39m# model attribute accordingly, if it was created from the model config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\transformers\\generation\\utils.py:1120\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m generate_compatible_classes:\n\u001b[0;32m   1119\u001b[0m     exception_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Please use one of the following classes instead: \u001b[39m\u001b[39m{\u001b[39;00mgenerate_compatible_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1120\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (BertForQuestionAnswering) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Who are you?\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=5)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up authentication credentials\n",
    "username = \"your_username\"\n",
    "password = \"your_password\"\n",
    "\n",
    "# Authenticate to the Confluence page\n",
    "auth_url = \"https://your-confluence-url.com/authenticate\"\n",
    "auth_payload = {\n",
    "    \"username\": username,\n",
    "    \"password\": password\n",
    "}\n",
    "auth_response = requests.post(auth_url, data=auth_payload)\n",
    "\n",
    "# Check if authentication was successful\n",
    "if auth_response.status_code == 200:\n",
    "    # Access the desired page URL\n",
    "    page_url = \"https://your-confluence-url.com/your-page\"\n",
    "    page_response = requests.get(page_url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    data = []\n",
    "\n",
    "    # Find and extract the relevant data from the page\n",
    "    for element in soup.find_all(\"p\"):\n",
    "        sentence = element.get_text().strip()\n",
    "        data.append(sentence)\n",
    "\n",
    "    # Print the scraped data\n",
    "    for sentence in data:\n",
    "        print(sentence)\n",
    "else:\n",
    "    print(\"Authentication failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   32, 47385,   318,  ...,     0,     0,     0],\n",
      "        [  198,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 7594, 42465, 17019,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  198,     0,     0,  ...,     0,     0,     0],\n",
      "        [41762,   364,   318,  ...,     0,     0,     0],\n",
      "        [  198,     0,     0,  ...,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_27364\\3092901424.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.data[index])\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     90\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 91\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     92\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     94\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\autograd\\function.py:264\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m    265\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    268\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "# Read the text file and extract training sentences\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.readlines()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "encoded_data = [tokenizer.encode(text) for text in train_data]\n",
    "\n",
    "# Pad or truncate the sequences to the same length\n",
    "padded_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in encoded_data], batch_first=True)\n",
    "\n",
    "print(padded_data)\n",
    "\n",
    "# Dataset Creation\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index])\n",
    "\n",
    "train_dataset = MyDataset(padded_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model Configuration\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "# Model architecture\n",
    "config.n_layer = 12  # Number of transformer layers\n",
    "config.n_head = 12  # Number of attention heads\n",
    "config.d_model = 768  # Dimensionality of the model's hidden states\n",
    "config.d_ffn = 3072  # Dimensionality of the feed-forward networks\n",
    "\n",
    "# Model input\n",
    "config.vocab_size = tokenizer.vocab_size  # Vocabulary size\n",
    "config.max_position_embeddings = 512  # Maximum length of input sequences\n",
    "\n",
    "# Regularization\n",
    "config.resid_pdrop = 0.1  # Dropout probability for residual connections\n",
    "config.embd_pdrop = 0.1  # Dropout probability for embedding layer\n",
    "config.attention_dropout_rate = 0.1  # Dropout rate for attention layers\n",
    "config.dropout_rate = 0.1  # Dropout rate for hidden layers\n",
    "\n",
    "# Other settings\n",
    "config.gradient_checkpointing = False  # Enable gradient checkpointing for memory optimization\n",
    "config.output_attentions = False  # Whether to output attentions weights\n",
    "config.output_hidden_states = False  # Whether to output hidden states of all layers\n",
    "\n",
    "# Customized parameters\n",
    "# Add your own customizations here\n",
    "\n",
    "# Example: Change the model's hidden size\n",
    "config.d_model = 1024\n",
    "\n",
    "# Example: Adjust the number of attention heads\n",
    "config.n_head = 16\n",
    "\n",
    "# Example: Enable gradient checkpointing\n",
    "config.gradient_checkpointing = True\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "num_epochs = 10\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[:, :-1].to(device)\n",
    "        target_ids = batch[:, 1:].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {average_loss}\")\n",
    "\n",
    "# Saving the trained model\n",
    "model.save_pretrained(\"my-trained-model\")\n",
    "tokenizer.save_pretrained(\"my-trained-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-24 13:48:20,388]\u001b[0m A new study created in memory with name: no-name-69f68b55-caa1-42a7-b2e0-ef6fe6247c71\u001b[0m\n",
      "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_27364\\3434032733.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.data[index])\n",
      "\u001b[33m[W 2023-05-24 13:51:58,330]\u001b[0m Trial 0 failed with parameters: {'n_layer': 11, 'd_ffn': 3072, 'resid_pdrop': 0.3399346351320539, 'embd_pdrop': 0.141232494717199, 'attention_dropout_rate': 0.437877700340633, 'dropout_rate': 0.2908560311157148, 'gradient_checkpointing': True} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_27364\\3434032733.py\", line 72, in objective\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-24 13:51:58,332]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m# Create an Optuna study\u001b[39;00m\n\u001b[0;32m     96\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     99\u001b[0m \u001b[39m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m    100\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[13], line 72\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     69\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     71\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 72\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     73\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "import optuna\n",
    "\n",
    "# Read the text file and extract training sentences\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.readlines()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "encoded_data = [tokenizer.encode(text) for text in train_data]\n",
    "\n",
    "# Pad or truncate the sequences to the same length\n",
    "padded_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in encoded_data], batch_first=True)\n",
    "\n",
    "# Dataset Creation\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index])\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Model Configuration\n",
    "    config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "    config.n_layer = trial.suggest_int(\"n_layer\", 6, 12)\n",
    "#    config.n_head = trial.suggest_int(\"n_head\", 6, 12)\n",
    "#    config.d_model = trial.suggest_categorical(\"d_model\", [512, 768, 1024])\n",
    "\n",
    "\n",
    "\n",
    "    config.d_ffn = trial.suggest_categorical(\"d_ffn\", [2048, 3072, 4096])\n",
    "    config.resid_pdrop = trial.suggest_float(\"resid_pdrop\", 0.0, 0.5)\n",
    "    config.embd_pdrop = trial.suggest_float(\"embd_pdrop\", 0.0, 0.5)\n",
    "    config.attention_dropout_rate = trial.suggest_float(\"attention_dropout_rate\", 0.0, 0.5)\n",
    "    config.dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    config.gradient_checkpointing = trial.suggest_categorical(\"gradient_checkpointing\", [True, False])\n",
    "    \n",
    "    model = GPT2LMHeadModel(config)\n",
    "    \n",
    "    train_dataset = MyDataset(padded_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    num_epochs = 200\n",
    "    # Training Loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        best_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            target_ids = batch[:, 1:].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        trial.report(average_loss, epoch=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Average Loss: {average_loss}\")\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= 3:\n",
    "                print(\"Early Stop Encounter as Loss did not improved.\")\n",
    "                break\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "model.save_pretrained(\"my-trained-model\")\n",
    "tokenizer.save_pretrained(\"my-trained-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third sentence sentence sentence sentence sentence sentence sentence sentence sentence\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"my-trained-model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"my-trained-model\")\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Third\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text using the model\n",
    "output_ids = model.generate(input_ids, max_length=10, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA_ModX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
