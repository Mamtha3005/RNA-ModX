{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software engineer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Example question and context\n",
    "question = \"Who is Shashi?\"\n",
    "context = \"Shashi is software engineer.\"\n",
    "\n",
    "# Encode the inputs \n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "# Get the predicted answer\n",
    "output = model(**inputs)\n",
    "\n",
    "# Get the most likely beginning and end of answer with the argmax of the score\n",
    "answer_start = torch.argmax(output.start_logits)  \n",
    "answer_end = torch.argmax(output.end_logits) + 1  \n",
    "\n",
    "# Get the answer\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "# Print the answer\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\n",
    "        \"context\": \"Shashi is software engineer.\",\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"id\": \"00001\",\n",
    "                \"question\": \"Who is Shashi?\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": \"software engineer\",\n",
    "                        \"answer_start\": 12\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:35<00:00, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 35.4116, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.085, 'train_loss': 12.04428482055664, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=12.04428482055664, metrics={'train_runtime': 35.4116, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.085, 'train_loss': 12.04428482055664, 'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# You have a list of dictionaries, so let's convert it to a dictionary of lists\n",
    "train_data_lists = {\n",
    "    \"context\": [\"Shashi is software engineer.\"],\n",
    "    \"question\": [\"Who is Shashi?\"],\n",
    "    \"answers\": [[{\"text\": \"software engineer\", \"answer_start\": 12}]]\n",
    "}\n",
    "\n",
    "valid_data_lists = {\n",
    "    \"context\": [\"Shashi lives in New York\"],\n",
    "    \"question\": [\"Where does he lives\"],\n",
    "    \"answers\": [[{\"text\": \"New York\", \"answer_start\": 14}]]\n",
    "}\n",
    "\n",
    "# Convert train_data to a Dataset object\n",
    "train_dataset = Dataset.from_dict(train_data_lists)\n",
    "valid_dataset = Dataset.from_dict(valid_data_lists)\n",
    "\n",
    "# Define a function to tokenize our examples\n",
    "def tokenize_examples(example):\n",
    "    # Encode our concatenated data\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        example[\"question\"], \n",
    "        example[\"context\"],\n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    # This is the tricky part, 'answer_start' is a character position. We need to find which token this position belongs to\n",
    "    # We can do that with the built in char_to_token method in the tokenizer\n",
    "    start_position = tokenizer(example['context'], return_offsets_mapping=True, truncation=True, max_length=512).char_to_token(example['answers'][0]['answer_start'])\n",
    "    end_position = start_position + len(example['answers'][0]['text'].split()) - 1\n",
    "\n",
    "    if start_position is None or end_position is None:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "\n",
    "    encoded.update({'start_positions': start_position, 'end_positions': end_position})\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Apply the function to our train_dataset\n",
    "train_dataset = train_dataset.map(tokenize_examples)\n",
    "valid_dataset = valid_dataset.map(tokenize_examples)\n",
    "\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (BertForQuestionAnswering) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mWho are you?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m output\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\transformers\\generation\\utils.py:1246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         synced_gpus \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[39m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 1246\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_class()\n\u001b[0;32m   1248\u001b[0m \u001b[39m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m generation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# legacy: users may modify the model configuration to control generation -- update the generation config\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39m# model attribute accordingly, if it was created from the model config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\transformers\\generation\\utils.py:1120\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m generate_compatible_classes:\n\u001b[0;32m   1119\u001b[0m     exception_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Please use one of the following classes instead: \u001b[39m\u001b[39m{\u001b[39;00mgenerate_compatible_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1120\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (BertForQuestionAnswering) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BertLMHeadModel'}"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Who are you?\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=5)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up authentication credentials\n",
    "username = \"your_username\"\n",
    "password = \"your_password\"\n",
    "\n",
    "# Authenticate to the Confluence page\n",
    "auth_url = \"https://your-confluence-url.com/authenticate\"\n",
    "auth_payload = {\n",
    "    \"username\": username,\n",
    "    \"password\": password\n",
    "}\n",
    "auth_response = requests.post(auth_url, data=auth_payload)\n",
    "\n",
    "# Check if authentication was successful\n",
    "if auth_response.status_code == 200:\n",
    "    # Access the desired page URL\n",
    "    page_url = \"https://your-confluence-url.com/your-page\"\n",
    "    page_response = requests.get(page_url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    data = []\n",
    "\n",
    "    # Find and extract the relevant data from the page\n",
    "    for element in soup.find_all(\"p\"):\n",
    "        sentence = element.get_text().strip()\n",
    "        data.append(sentence)\n",
    "\n",
    "    # Print the scraped data\n",
    "    for sentence in data:\n",
    "        print(sentence)\n",
    "else:\n",
    "    print(\"Authentication failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   32, 47385,   318,  ...,     0,     0,     0],\n",
      "        [  198,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 7594, 42465, 17019,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  198,     0,     0,  ...,     0,     0,     0],\n",
      "        [41762,   364,   318,  ...,     0,     0,     0],\n",
      "        [  198,     0,     0,  ...,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_27364\\3100729983.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.data[index])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "# Read the text file and extract training sentences\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = file.readlines()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "encoded_data = [tokenizer.encode(text) for text in train_data]\n",
    "\n",
    "# Pad or truncate the sequences to the same length\n",
    "padded_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in encoded_data], batch_first=True)\n",
    "\n",
    "print(padded_data)\n",
    "\n",
    "# Dataset Creation\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index])\n",
    "\n",
    "train_dataset = MyDataset(padded_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model Configuration\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel(config)\n",
    "num_epochs = 10\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[:, :-1].to(device)\n",
    "        target_ids = batch[:, 1:].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {average_loss}\")\n",
    "\n",
    "# Saving the trained model\n",
    "model.save_pretrained(\"my-trained-model\")\n",
    "tokenizer.save_pretrained(\"my-trained-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third sentence sentence sentence sentence sentence sentence sentence sentence sentence\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"my-trained-model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"my-trained-model\")\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Third\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text using the model\n",
    "output_ids = model.generate(input_ids, max_length=10, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA_ModX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
