{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkDJ1UJK69ON",
        "outputId": "c1eec5ea-65de-40a5-9c79-41a472c445ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/390.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.16)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.6.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna  scikit-learn gensim imbalanced-learn xgboost torch pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ohWCGHSq5BVM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        " #Import All Libraries Here\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import time\n",
        "from collections import Counter\n",
        "# PyTorch Import\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 50\n",
        "\n",
        "# 1 - One Hot Encoding with Pytorch in build Emnedding\n",
        "# 2 - 3-mer coding with number encoding\n",
        "ENCODING_METHOD = 2\n",
        "\n",
        "# 1- Random Over Sampling\n",
        "# 2 - Weighted Over Sampler\n",
        "SAMPLING_METHOD =1\n",
        "\n",
        "# 1 - LSTM with Cross Entropy\n",
        "MODEL = 1\n",
        "\n",
        "\n",
        "FRAMEWORK = \"PYTORCH\"\n",
        "\n",
        "# Startegy to Crop Sequene\n",
        "# MID - Modification is present at Mid of cropped Sequence\n",
        "# END - Modification is present at End of cropepd Sequence\n",
        "CROP_STRATEGY = 'MID'\n",
        "\n",
        "# Y Category Encoding Method\n",
        "# LABEL or ONE_HOT\n",
        "TARGET_ENCODING = 'LABEL'\n",
        "\n",
        "ENCODING_FILE = '3-mer-dictionary.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQvMm5lH5BVO",
        "outputId": "442f7e0b-129a-4add-f95e-c353acecda48"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# INPUT_TRAIN_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/train_in.csv\"\n",
        "# INPUT_TRAIN_OUT = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/train_out.csv\"\n",
        "# INPUT_TEST_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/test_in.csv\"\n",
        "# INPUT_TEST_OUT = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/test_out.csv\"\n",
        "# INPUT_VALIDATION_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/valid_in_nucleo.csv\"\n",
        "# INPUT_VALIDATION_OUT  = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/valid_out.csv\"\n",
        "\n",
        "# Record Constants\n",
        "# Record Constants\n",
        "INPUT_TRAIN_IN = \"../../../data/train_in.csv\"\n",
        "INPUT_TRAIN_OUT = \"../../../data/train_out.csv\"\n",
        "INPUT_TEST_IN = \"../../../data/test_in.csv\"\n",
        "INPUT_TEST_OUT = \"../../../data/test_out.csv\"\n",
        "INPUT_VALIDATION_IN = \"../../../data/valid_in_nucleo.csv\"\n",
        "INPUT_VALIDATION_OUT  = \"../../../data/valid_out.csv\"\n",
        "\n",
        "# TARGET_MODEL_PATH = '../../webapp/model_files'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcUOufcj5BVO",
        "outputId": "10cc65ec-1618-4a0f-dcc7-0727a3a71f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Shape of X : (304661, 1001) and Tranin Shape of Y : (304661, 1001)\n",
            "Test Shape of X : (1200, 1001) and Test Shape of Y : (1200, 12)\n",
            "Validation Shape of X : (3599, 1001) and Validation Shape of Y : (3599, 12)\n"
          ]
        }
      ],
      "source": [
        "#Read X Varaibles and Y Varaibles\n",
        "\n",
        "x_train_raw =  pd.read_csv(INPUT_TRAIN_IN, header=None , skiprows=1 )\n",
        "y_train_raw =  pd.read_csv(INPUT_TRAIN_OUT, header=None , skiprows=1 )\n",
        "\n",
        "x_test_raw =  pd.read_csv(INPUT_TEST_IN, header=None , skiprows=1 )\n",
        "y_test_raw =  pd.read_csv(INPUT_TEST_OUT, header=None , skiprows=1)\n",
        "\n",
        "x_valid_raw =  pd.read_csv(INPUT_VALIDATION_IN, header=None , skiprows=1 )\n",
        "y_valid_raw =  pd.read_csv(INPUT_VALIDATION_OUT, header=None , skiprows=1 )\n",
        "\n",
        "x_data = pd.concat([x_train_raw, x_test_raw, x_valid_raw], axis=0, ignore_index=True)\n",
        "y_data = pd.concat([y_train_raw, y_test_raw, y_valid_raw], axis=0, ignore_index=True)\n",
        "\n",
        "print(f\"Train Shape of X : {x_train_raw.shape} and Tranin Shape of Y : {x_train_raw.shape}\")\n",
        "print(f\"Test Shape of X : {x_test_raw.shape} and Test Shape of Y : {y_test_raw.shape}\")\n",
        "print(f\"Validation Shape of X : {x_valid_raw.shape} and Validation Shape of Y : {y_valid_raw.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwZ2AFCo5BVO"
      },
      "source": [
        "### Calculate Sequence Positions to extracted from Original Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uWqxeK7J5BVP"
      },
      "outputs": [],
      "source": [
        "middle_index = (x_train_raw.shape[1] // 2) + 1 # This is location for Modified Sequence . Use this as Y Target\n",
        "\n",
        "if CROP_STRATEGY == 'MID':\n",
        "    STRAT_INEDX =middle_index - WINDOW_SIZE -1\n",
        "    END_INDEX =middle_index + WINDOW_SIZE\n",
        "\n",
        "if CROP_STRATEGY == 'END':\n",
        "    STRAT_INEDX =middle_index - (WINDOW_SIZE*2) -1\n",
        "    END_INDEX =middle_index\n",
        "\n",
        "x_data_cropped =  x_data.iloc[:,STRAT_INEDX :END_INDEX]\n",
        "concatenated_column= x_data_cropped.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
        "x_data_cropped = x_data_cropped.assign(Sequence=concatenated_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "jRDnwbiM5BVP",
        "outputId": "0d007b12-6da3-4cca-814a-57e8a3c5e5ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>450</th>\n",
              "      <th>451</th>\n",
              "      <th>452</th>\n",
              "      <th>453</th>\n",
              "      <th>454</th>\n",
              "      <th>455</th>\n",
              "      <th>456</th>\n",
              "      <th>457</th>\n",
              "      <th>458</th>\n",
              "      <th>459</th>\n",
              "      <th>...</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCAT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>TTTGAAAAAATATTAGCAATGTGAGGACACTTAAGCAGTTTTGTCA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>AGAAACATTCAACCTCCCTTCTTTTTATTCCAGTTGTCCTTTTCTC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>TTAGTTTTACTATGGAATCATAATAACCCACATAGAAGACTGATAT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>CAACAGAAGTTTCTCATCTATAATCAGTAGCACTAAACTCTTGGTT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309455</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>CCAAACTCTTTATCTCTTGAGTTCTCAGCCAATAGGGCCATTGTAG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309456</th>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>GATCCAGTTGAAAACGTATCCCTCTACTTTCTTCAGTTGTAGAAAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309457</th>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>T</td>\n",
              "      <td>GCCAGGGCAAAGCTGGCTGATTTTACGTGTTTAAGGATGAAATATC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309458</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>CTGGGTGCGACAGGCCACTGGACAAGGGCTTGAGTGGATGGGATGG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309459</th>\n",
              "      <td>G</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>...</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>GGCTGCTAAGGCAATGTGCTCTCCTAATTTCCCTTTTTCCTTTGTG...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>309460 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       450 451 452 453 454 455 456 457 458 459  ... 542 543 544 545 546 547  \\\n",
              "0        T   T   G   C   C   A   C   A   C   T  ...   C   A   G   T   A   T   \n",
              "1        T   T   T   G   A   A   A   A   A   A  ...   T   C   A   T   C   G   \n",
              "2        A   G   A   A   A   C   A   T   T   C  ...   T   T   C   T   G   T   \n",
              "3        T   T   A   G   T   T   T   T   A   C  ...   A   A   A   A   A   T   \n",
              "4        C   A   A   C   A   G   A   A   G   T  ...   A   A   A   A   T   G   \n",
              "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
              "309455   C   C   A   A   A   C   T   C   T   T  ...   G   G   G   C   A   G   \n",
              "309456   G   A   T   C   C   A   G   T   T   G  ...   A   C   A   G   G   T   \n",
              "309457   G   C   C   A   G   G   G   C   A   A  ...   C   A   A   G   C   T   \n",
              "309458   C   T   G   G   G   T   G   C   G   A  ...   G   C   A   G   A   G   \n",
              "309459   G   G   C   T   G   C   T   A   A   G  ...   C   T   C   A   A   A   \n",
              "\n",
              "       548 549 550                                           Sequence  \n",
              "0        C   T   C  TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCAT...  \n",
              "1        T   G   C  TTTGAAAAAATATTAGCAATGTGAGGACACTTAAGCAGTTTTGTCA...  \n",
              "2        T   C   A  AGAAACATTCAACCTCCCTTCTTTTTATTCCAGTTGTCCTTTTCTC...  \n",
              "3        T   T   C  TTAGTTTTACTATGGAATCATAATAACCCACATAGAAGACTGATAT...  \n",
              "4        T   A   C  CAACAGAAGTTTCTCATCTATAATCAGTAGCACTAAACTCTTGGTT...  \n",
              "...     ..  ..  ..                                                ...  \n",
              "309455   A   G   A  CCAAACTCTTTATCTCTTGAGTTCTCAGCCAATAGGGCCATTGTAG...  \n",
              "309456   A   A   T  GATCCAGTTGAAAACGTATCCCTCTACTTTCTTCAGTTGTAGAAAA...  \n",
              "309457   G   A   T  GCCAGGGCAAAGCTGGCTGATTTTACGTGTTTAAGGATGAAATATC...  \n",
              "309458   T   C   A  CTGGGTGCGACAGGCCACTGGACAAGGGCTTGAGTGGATGGGATGG...  \n",
              "309459   C   G   A  GGCTGCTAAGGCAATGTGCTCTCCTAATTTCCCTTTTTCCTTTGTG...  \n",
              "\n",
              "[309460 rows x 102 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data_cropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KLuiCU2X5BVP"
      },
      "outputs": [],
      "source": [
        "x_train_raw = None\n",
        "y_train_raw = None\n",
        "x_test_raw = None\n",
        "y_test_raw = None\n",
        "x_valid_raw = None\n",
        "y_valid_raw = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIM49qM75BVP"
      },
      "source": [
        "### Apply One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tf17dHN95BVP"
      },
      "outputs": [],
      "source": [
        "number_of_unique_kmers = set()\n",
        "def encode_seq(kmer_token):\n",
        "\n",
        "    # A 1 0 0 0\n",
        "    # C 0 1 0 0\n",
        "    # T/U 0 0 0 1\n",
        "    # G 0 0 1 0\n",
        "    # N 0 0 0 0\n",
        "\n",
        "    encoding_dict = {\n",
        "        'A': [1, 0, 0, 0],\n",
        "        'C': [0, 1, 0, 0],\n",
        "        'G': [0, 0, 1, 0],\n",
        "        'T': [0, 0, 0, 1],\n",
        "        'U': [0, 0, 0, 1],\n",
        "        'N': [0, 0, 0, 0],\n",
        "    }\n",
        "\n",
        "    encoded_sequence = []\n",
        "    number_of_unique_kmers.add(kmer_token)\n",
        "    for  base in kmer_token:\n",
        "        encoded_sequence.append(encoding_dict[base])\n",
        "    return np.array(encoded_sequence).flatten()\n",
        "\n",
        "def applyOneHotEncoding(tokenized_sequences):\n",
        "    encoded_sequences = []\n",
        "    for seq in tokenized_sequences:\n",
        "        encoded_sequences.append(encode_seq(seq))\n",
        "\n",
        "    return np.array(encoded_sequences).flatten()\n",
        "\n",
        "def encode_with_one_hot_encoding(x_train_raw):\n",
        "    truncated_df =  x_train_raw.iloc[:,STRAT_INEDX :END_INDEX] # Window Starts from V501 with 50 window size\n",
        "    concatenated_column= truncated_df.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
        "    df_result = truncated_df.assign(Sequence=concatenated_column)\n",
        "    tokenized_sequences =  df_result['Sequence'].apply(applyOneHotEncoding).tolist()\n",
        "\n",
        "    return tokenized_sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk_WaSe35BVP"
      },
      "source": [
        "### 3 mer coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JwIzW5gY5BVQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "kmer_dict = {}\n",
        "k = 3\n",
        "with open(ENCODING_FILE, 'rb') as f:\n",
        "    kmer_dict = pickle.load(f)\n",
        "\n",
        "\n",
        "def encode_with_k_mer_codon(sequence):\n",
        "    encoded_sequence = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        encoded_sequence.append(kmer_dict[sequence[i:i+k]] )\n",
        "\n",
        "    return np.array(encoded_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ds7NWJtl5BVQ"
      },
      "outputs": [],
      "source": [
        "## Filter Dataset to Keep only Target Binary Class\n",
        "\n",
        "RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
        "RMEncoding = [12,1,2,3,4,5,6,7,8,9,10,11,0]\n",
        "\n",
        "ARMs = ['hm6A', 'hm1A' , 'hAm' , 'Atol' , 'hm6Am']\n",
        "ARMEncoding = [0,1,2,3,4]\n",
        "\n",
        "def convert_y_to_original_labels(row):\n",
        "    label = \"\"\n",
        "    for index , n in enumerate(row.tolist()) :\n",
        "        if n == 1 :\n",
        "            label = RMs[index]\n",
        "    if label == '':\n",
        "        return 'NonMoD'\n",
        "    return label\n",
        "\n",
        "def get_original_y_lables( y_data ):\n",
        "    # Convert One Hot Encoded Y to to Original Labels\n",
        "    y_original_labels = y_data.apply(convert_y_to_original_labels,axis=1)\n",
        "    return y_original_labels\n",
        "\n",
        "\n",
        "\n",
        "def encode_target(y_data):\n",
        "    # Write Customer Lable Encoder . This is required since we have train and test alreday splitted. Always creating a new instanc of label encoder will change encoding.\n",
        "\n",
        "    y_encoded = []\n",
        "    for y in y_data:\n",
        "        index = ARMs.index(y)\n",
        "        encoding =  ARMEncoding[index]\n",
        "        y_encoded.append(encoding)\n",
        "    return y_encoded\n",
        "\n",
        "def prepare_data_for_binary_classification(x_data , y_data , prediction_class):\n",
        "    # Convert One Hot Encoded Y to to Original Labels\n",
        "    y_original_labels = y_data.apply(convert_y_to_original_labels,axis=1)\n",
        "    x_data['Label'] = y_original_labels\n",
        "    target_class = prediction_class\n",
        "    selected_rna_data = x_data[x_data['Label'].isin(target_class)]\n",
        "\n",
        "    y_filtered = selected_rna_data['Label']\n",
        "    x_filtered = selected_rna_data.drop('Label', axis=1)\n",
        "\n",
        "    return x_filtered , y_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zJEu1A4c5BVQ"
      },
      "outputs": [],
      "source": [
        "class_A_list = ['hm6A', 'hm1A' , 'hAm' , 'Atol' , 'hm6Am']\n",
        "x_data_filtered , y_data_filtered = prepare_data_for_binary_classification(x_data_cropped , y_data , class_A_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akLbxB6z5BVQ",
        "outputId": "faaf160a-ff4f-4f06-f16d-29667a37b997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(138175, 102)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "EJjCTy9m5BVQ",
        "outputId": "a5b4a34b-0f23-4ff6-cb26-b9aa16851f7e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "y_counts = y_data_filtered.value_counts()\n",
        "colors = cm.tab20(range(len(y_counts)))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(y_counts.index, y_counts.values , color = colors)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Value counts of each class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTPaFg0d5BVQ",
        "outputId": "794c6077-0e7c-4358-89ff-7d6c7039f921"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCAT...\n",
              "1         TTTGAAAAAATATTAGCAATGTGAGGACACTTAAGCAGTTTTGTCA...\n",
              "2         AGAAACATTCAACCTCCCTTCTTTTTATTCCAGTTGTCCTTTTCTC...\n",
              "3         TTAGTTTTACTATGGAATCATAATAACCCACATAGAAGACTGATAT...\n",
              "4         CAACAGAAGTTTCTCATCTATAATCAGTAGCACTAAACTCTTGGTT...\n",
              "                                ...                        \n",
              "309305    TTAAAACTGTATCACACAAAACTATTCTTAACTTCAAGAAAGAAGC...\n",
              "309306    TCATTTCTTTTGTCTAGTCACTTCTCCATAATATATCACACTTTGT...\n",
              "309307    AAACTCTAACTAGAAATCCACTTTCCAGAGGCTGTCTCAGCAAGTG...\n",
              "309308    AGTCATTTTGGCTGGGTTTGTTCCTGGCTCTGCCACTTATGAGGTT...\n",
              "309309    AGCAGTAGAAAGAAGTCCCTGCATCCACCTCAGTAGGTTTACATGG...\n",
              "Name: Sequence, Length: 138175, dtype: object"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data_filtered['Sequence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULG0B0e95BVQ",
        "outputId": "69c76a6c-6b7e-4121-9a88-96d17bf2a5b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_6624\\1490553768.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
            "  X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n"
          ]
        }
      ],
      "source": [
        "X_encoded  =  x_data_filtered['Sequence'].apply(encode_with_k_mer_codon)\n",
        "X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n",
        "y_encoded = encode_target(y_data_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne-hmuDd5BVR",
        "outputId": "dbebeca1-6b70-4cfd-b2d2-3c011fdfac48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[35, 28, 29,  ..., 12, 22, 14],\n",
              "        [50, 35, 52,  ..., 49, 27, 28],\n",
              "        [56, 53, 63,  ..., 34, 43, 11],\n",
              "        ...,\n",
              "        [63, 59, 58,  ...,  9, 27, 52],\n",
              "        [ 9, 10, 11,  ..., 34, 62, 47],\n",
              "        [31, 18, 19,  ..., 43, 11, 36]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etAEiUuL5BVR",
        "outputId": "3ce21d47-4199-4911-83b0-a095d1ceda83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate Train and Split..\n"
          ]
        }
      ],
      "source": [
        "print(\"Generate Train and Split..\")\n",
        "# Train set\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test and Validation set\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxuA3OdC5BVR",
        "outputId": "0df4344b-8c3a-4fbc-c67b-4973076f8446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Y Count :  Counter({0: 45515, 3: 36874, 1: 11487, 4: 1757, 2: 1089})\n",
            "Test Y Count :  Counter({0: 9830, 3: 7888, 1: 2427, 4: 326, 2: 256})\n"
          ]
        }
      ],
      "source": [
        "X_encoded = None\n",
        "y_encoded = None\n",
        "x_data_filtered , y_data_filtered = None,None\n",
        "x_data , y_data = None , None\n",
        "\n",
        "print(\"Train Y Count : \" ,Counter(y_train))\n",
        "print(\"Test Y Count : \" ,Counter(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zb-kIy35BVR"
      },
      "source": [
        "### Balance Datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3tEqKJG25BVR"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(y_train , dtype=torch.long)\n",
        "y_test = torch.tensor(y_test , dtype=torch.long)\n",
        "y_valid = torch.tensor(y_valid , dtype=torch.long)\n",
        "\n",
        "sm = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
        "y_resampled = torch.tensor(y_resampled , dtype=torch.float32) # Keeping float32\n",
        "X_resampled = torch.tensor(X_resampled , dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpftneeP5BVR",
        "outputId": "45c7f234-7cba-475a-85a2-a23c25c9aa29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([96722, 99])\n",
            "torch.Size([96722])\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM0C9uYE5BVS",
        "outputId": "3cb2f8f6-abd9-44f5-81d6-5d71e9a1162b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([19.,  9., 10., 22., 42., 35., 26., 10., 22., 25., 45., 40., 40., 51.,\n",
              "        39., 20., 40., 51., 55.,  3., 32., 61., 50., 43., 22.,  7., 47., 48.,\n",
              "        20., 40., 51., 55., 61., 62., 47., 63., 48., 20., 51., 57., 37., 19.,\n",
              "        31., 29.,  1.,  2., 12., 22.,  7., 47., 48.,  9., 10., 22., 42., 35.,\n",
              "        45., 17., 24.,  7., 32.,  3., 47., 59., 37., 60., 48., 20., 40., 21.,\n",
              "        10., 11.,  2., 12., 11., 60., 48.,  9., 27., 52., 39., 20., 17., 29.,\n",
              "         0.,  6., 25., 26., 10., 11., 19., 31., 18., 60., 54.,  3., 32., 33.,\n",
              "        52.])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_resampled[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pIOzPJWY5BVS"
      },
      "outputs": [],
      "source": [
        "hyperparameter = {}\n",
        "hyperparameter['INPUT_DIMENSION'] = len(kmer_dict) # For One Hot Encoding Input Dimension would be 4 as there only 4 unique nucleocide\n",
        "hyperparameter['HIDDEN_DIMENSION'] = 32\n",
        "hyperparameter['NO_OF_LAYERS'] = 2\n",
        "hyperparameter['BATCH_SIZE'] = 32\n",
        "hyperparameter['OUTPUT_DIMENSION'] = 5\n",
        "hyperparameter['EMBEDDING_DIMENSION'] = 16 # if you are using Word2Vec Encoding then this should be same as Word2Vec Embedding Dim\n",
        "hyperparameter['DROP_OUT'] = 0.1\n",
        "hyperparameter['LEARNING_RATE'] = 0.0001\n",
        "\n",
        "\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "train_dataset = RNADataset(X_resampled, y_resampled)\n",
        "test_dataset = RNADataset(X_test, y_test)\n",
        "valid_dataset = RNADataset(X_valid, y_valid)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sXVdBroi5BVS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNATransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.1):\n",
        "        super(RNATransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        # If batch size first is true then it should be batch size , sequence lenght , embedding dimension\n",
        "        self.encoder_layer_1 = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=hidden_dim , batch_first=True)\n",
        "        self.transformer_encoder_1 = nn.TransformerEncoder(self.encoder_layer_1, num_layers=num_layers)\n",
        "\n",
        "        self.encoder_layer_2 = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=hidden_dim , batch_first=True)\n",
        "        self.transformer_encoder_2 = nn.TransformerEncoder(self.encoder_layer_2, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        x_embedded = self.embedding(x)\n",
        "        #print(\"Shape of X embedded\" , x_embedded.shape)\n",
        "        x_transformed = self.transformer_encoder_1(x_embedded)\n",
        "\n",
        "        #print(x_transformed.shape)\n",
        "        #x_transformed = x_transformed[:, -1, :]  # taking the last token's output\n",
        "\n",
        "        #print(\"Next 2\" , x_transformed.shape)\n",
        "        x_transformed_2 = self.transformer_encoder_2(x_transformed)\n",
        "\n",
        "        #print(\"Layer 2 Shaop\" , x_transformed_2.shape)\n",
        "        x_transformed_2 = x_transformed_2[:, -1, :]  # taking the last token's output\n",
        "\n",
        "\n",
        "        output = self.dropout(x_transformed_2)\n",
        "        out = self.fc(output)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sP484k65BVS",
        "outputId": "1bf3a3f4-e539-4b89-8f0c-3918db6732ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 99])\n"
          ]
        }
      ],
      "source": [
        "# Check data is in correct shape - batch size , sequece len , embedding dimension size\n",
        "for inputs, labels in train_dataloader:\n",
        "    print(inputs.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-t8RdUvy5BVS"
      },
      "outputs": [],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "\n",
        "def validate_model(model, test_dataloader , device ,loss_function):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    class_correct = [0] * hyperparameter['OUTPUT_DIMENSION']\n",
        "    class_total = [0] * hyperparameter['OUTPUT_DIMENSION']\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())  # Capture True Lables for Summary Report\n",
        "            predicted_labels.extend(predicted.cpu().numpy()) # Capture Predicted Labels Lables for Summary Report\n",
        "\n",
        "    validation_loss = running_loss / len(test_dataloader)\n",
        "    validation_accuracy = correct / total\n",
        "\n",
        "    return validation_loss , validation_accuracy , true_labels , predicted_labels\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, device, epochs, optimizer, loss_function):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        val_loss,  validation_accuracy , true_labels , predicted_labels = validate_model(model, test_dataloader, device, loss_function)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Test Accuracy: {validation_accuracy:.4f} ,Learning Rate: {optimizer.param_groups[0]['lr']} , Time Taken : {elapsed_time}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count == 5:\n",
        "                print(\"No improvement in validation loss for 5 epochs. Training stopped.\")\n",
        "                break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7wtvOzF5BVS",
        "outputId": "ea97784b-22dc-4b48-f2c6-30f3f397fce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Number of Parameters for Model Training : 14933 \n",
            "Model Parameters  :  {'INPUT_DIMENSION': 94, 'HIDDEN_DIMENSION': 32, 'NO_OF_LAYERS': 2, 'BATCH_SIZE': 32, 'OUTPUT_DIMENSION': 5, 'EMBEDDING_DIMENSION': 16, 'DROP_OUT': 0.1, 'LEARNING_RATE': 0.0001}\n"
          ]
        }
      ],
      "source": [
        "model = RNATransformerModel(input_dim=hyperparameter['INPUT_DIMENSION'],\n",
        "                            embedding_dim=hyperparameter['EMBEDDING_DIMENSION'],\n",
        "                            hidden_dim=hyperparameter['HIDDEN_DIMENSION'] ,\n",
        "                            num_layers = hyperparameter['NO_OF_LAYERS'],\n",
        "                            output_dim=hyperparameter['OUTPUT_DIMENSION'],\n",
        "                            dropout=hyperparameter['DROP_OUT'] )\n",
        "\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()  ## MSELoss of Regression problem  # BCELoss for binary classification\n",
        "optimizer = optim.Adam(model.parameters() ,  lr=hyperparameter['LEARNING_RATE'])\n",
        "\n",
        "# Number of Parameters for Model\n",
        "total_parameters = []\n",
        "for p in model.parameters():\n",
        "    total_parameters.append(p.numel())\n",
        "\n",
        "print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model Parameters  : \" , hyperparameter)\n",
        "\n",
        "# Train Model with configured Parameter\n",
        "train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optuna Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-08-09 11:55:04,334]\u001b[0m A new study created in memory with name: no-name-51af5fa7-8883-4f57-a1ab-78a467b8d875\u001b[0m\n",
            "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_3960\\2403567092.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  hyperparameter['LEARNING_RATE'] = trial.suggest_loguniform('LEARNING_RATE', 1e-5, 1e-3)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.6028, Val Loss: 1.5708, Test Accuracy: 0.2399 , Time Taken : 284.43349528312683\n",
            "Epoch 2, Train Loss: 1.5642, Val Loss: 1.5313, Test Accuracy: 0.3051 , Time Taken : 314.73081827163696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m[W 2023-08-09 12:06:45,445]\u001b[0m Trial 0 failed with parameters: {'HIDDEN_DIMENSION': 37, 'NO_OF_LAYERS': 1, 'BATCH_SIZE': 16, 'DROP_OUT': 0.23769099225558366, 'LEARNING_RATE': 0.00026782517683361266} because of the following error: KeyboardInterrupt().\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_3960\\2403567092.py\", line 31, in objective\n",
            "    train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)\n",
            "  File \"C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_3960\\1068412963.py\", line 47, in train_model\n",
            "    outputs = model(inputs)\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_3960\\3348702098.py\", line 20, in forward\n",
            "    x_transformed = self.transformer_encoder(x_embedded)\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 315, in forward\n",
            "    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 591, in forward\n",
            "    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 599, in _sa_block\n",
            "    x = self.self_attn(x, x, x,\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1205, in forward\n",
            "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
            "  File \"c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\functional.py\", line 5373, in multi_head_attention_forward\n",
            "    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
            "KeyboardInterrupt\n",
            "\u001b[33m[W 2023-08-09 12:06:45,472]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m final_accuracy\n\u001b[0;32m     38\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Print the result\u001b[39;00m\n\u001b[0;32m     42\u001b[0m trial \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[31], line 31\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     27\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mhyperparameter[\u001b[39m'\u001b[39m\u001b[39mBATCH_SIZE\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m valid_dataloader \u001b[39m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39mhyperparameter[\u001b[39m'\u001b[39m\u001b[39mBATCH_SIZE\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)\n\u001b[0;32m     33\u001b[0m _, final_accuracy, true_labels, predicted_labels \u001b[39m=\u001b[39m validate_model(model, valid_dataloader,device,loss_function)\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m final_accuracy\n",
            "Cell \u001b[1;32mIn[23], line 47\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, test_dataloader, device, epochs, optimizer, loss_function)\u001b[0m\n\u001b[0;32m     44\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 47\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     48\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, labels)\n\u001b[0;32m     49\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[26], line 20\u001b[0m, in \u001b[0;36mRNATransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m x_embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m     19\u001b[0m \u001b[39m#print(\"Shape of X embedded\" , x_embedded.shape)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m x_transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x_embedded)\n\u001b[0;32m     21\u001b[0m x_transformed \u001b[39m=\u001b[39m x_transformed[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# taking the last token's output\u001b[39;00m\n\u001b[0;32m     23\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x_transformed)\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[0;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:591\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    589\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39;49mis_causal))\n\u001b[0;32m    592\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    594\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 599\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    600\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    601\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    602\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, is_causal\u001b[39m=\u001b[39;49mis_causal)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[0;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[0;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
            "File \u001b[1;32mc:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\functional.py:5373\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5370\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5371\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 5373\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[0;32m   5374\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5376\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    hyperparameter = {}\n",
        "    hyperparameter['INPUT_DIMENSION'] = len(kmer_dict)\n",
        "    hyperparameter['HIDDEN_DIMENSION'] = trial.suggest_int('HIDDEN_DIMENSION', 16, 128)\n",
        "    hyperparameter['NO_OF_LAYERS'] = trial.suggest_int('NO_OF_LAYERS', 1, 4)\n",
        "    hyperparameter['BATCH_SIZE'] = trial.suggest_categorical('BATCH_SIZE', [16, 32, 64])\n",
        "    hyperparameter['OUTPUT_DIMENSION'] = 5\n",
        "    hyperparameter['EMBEDDING_DIMENSION'] = 16  # Adjust as needed\n",
        "    hyperparameter['DROP_OUT'] = trial.suggest_float('DROP_OUT', 0.1, 0.5)\n",
        "    hyperparameter['LEARNING_RATE'] = trial.suggest_loguniform('LEARNING_RATE', 1e-5, 1e-3)\n",
        "\n",
        "    model = RNATransformerModel(input_dim=hyperparameter['INPUT_DIMENSION'],\n",
        "                                embedding_dim=hyperparameter['EMBEDDING_DIMENSION'],\n",
        "                                hidden_dim=hyperparameter['HIDDEN_DIMENSION'],\n",
        "                                num_layers=hyperparameter['NO_OF_LAYERS'],\n",
        "                                output_dim=hyperparameter['OUTPUT_DIMENSION'],\n",
        "                                dropout=hyperparameter['DROP_OUT'])\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hyperparameter['LEARNING_RATE'])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "\n",
        "\n",
        "    train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)\n",
        "\n",
        "    _, final_accuracy, true_labels, predicted_labels = validate_model(model, valid_dataloader,device,loss_function)\n",
        "\n",
        "    return final_accuracy\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "    # Print the result\n",
        "trial = study.best_trial\n",
        "print('Accuracy: {}'.format(trial.value))\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8rs97X-5BVS",
        "outputId": "26c46104-dafe-484d-85e8-8d51bb5c00ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Accuracy: 0.5693\n",
            "\n",
            " Classification Summary:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80      9833\n",
            "           1       0.24      0.09      0.13      2427\n",
            "           2       0.02      0.09      0.04       246\n",
            "           3       0.76      0.17      0.28      7856\n",
            "           4       0.16      1.00      0.27       364\n",
            "\n",
            "    accuracy                           0.57     20726\n",
            "   macro avg       0.37      0.47      0.31     20726\n",
            "weighted avg       0.64      0.57      0.51     20726\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "_, final_accuracy, true_labels, predicted_labels = validate_model(model, valid_dataloader,device,loss_function)\n",
        "\n",
        "# Print the final accuracy\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Print the classification summary\n",
        "print(\"\\n Classification Summary:\")\n",
        "print(classification_report(true_labels, predicted_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c230947687a8f3ffad2ce5baec6aac89e01c839661a42aacfb7c80a5442a49ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
