{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhuWQTdS93Jn"
      },
      "source": [
        "# RNA 4 Nucleoside Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kZ_i_0-j93Jp"
      },
      "outputs": [],
      "source": [
        " #Import All Libraries Here\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import time\n",
        "from collections import Counter\n",
        "# PyTorch Import\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fDtq9jF-E8l",
        "outputId": "427a49fa-fcb1-4942-e0f2-ef6be18bbe1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.2-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/225.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.2 cmaes-0.10.0 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna  scikit-learn gensim imbalanced-learn xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6FRymYz97o7",
        "outputId": "35623297-54d4-4f29-c5d9-a5f17f4cb85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "INPUT_TRAIN_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/train_in.csv\"\n",
        "INPUT_TRAIN_OUT = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/train_out.csv\"\n",
        "INPUT_TEST_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/test_in.csv\"\n",
        "INPUT_TEST_OUT = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/test_out.csv\"\n",
        "INPUT_VALIDATION_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/valid_in_nucleo.csv\"\n",
        "INPUT_VALIDATION_OUT  = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/valid_out.csv\"\n",
        "\n",
        "# Record Constants\n",
        "# INPUT_TRAIN_IN = \"../../../data/train_in.csv\"\n",
        "# INPUT_TRAIN_OUT = \"../../../data/train_out.csv\"\n",
        "# INPUT_TEST_IN = \"../../../data/test_in.csv\"\n",
        "# INPUT_TEST_OUT = \"../../../data/test_out.csv\"\n",
        "# INPUT_VALIDATION_IN = \"../../../data/valid_in_nucleo.csv\"\n",
        "# INPUT_VALIDATION_OUT  = \"../../../data/valid_out.csv\"\n",
        "\n",
        "TARGET_MODEL_PATH = '../../webapp/model_files'\n",
        "ENCODING_FILE = '3-mer-dictionary.pkl'\n",
        "\n",
        "WINDOW_SIZE = 50\n",
        "\n",
        "# Startegy to Crop Sequene\n",
        "# MID - Modification is present at Mid of cropped Sequence\n",
        "# END - Modification is present at End of cropepd Sequence\n",
        "CROP_STRATEGY = 'MID'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7HMGVZeh93Jq"
      },
      "outputs": [],
      "source": [
        "#Read X Varaibles and Y Varaibles\n",
        "\n",
        "x_train_raw =  pd.read_csv(INPUT_TRAIN_IN, header=None , skiprows=1  )\n",
        "y_train_raw =  pd.read_csv(INPUT_TRAIN_OUT, header=None , skiprows=1 )\n",
        "\n",
        "x_test_raw =  pd.read_csv(INPUT_TEST_IN, header=None , skiprows=1 )\n",
        "y_test_raw =  pd.read_csv(INPUT_TEST_OUT, header=None , skiprows=1)\n",
        "\n",
        "x_valid_raw =  pd.read_csv(INPUT_VALIDATION_IN, header=None , skiprows=1 )\n",
        "y_valid_raw =  pd.read_csv(INPUT_VALIDATION_OUT, header=None , skiprows=1 )\n",
        "\n",
        "\n",
        "x_data = pd.concat([x_train_raw, x_test_raw, x_valid_raw], axis=0, ignore_index=True)\n",
        "y_data = pd.concat([y_train_raw, y_test_raw, y_valid_raw], axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CkcDtmho93Jr"
      },
      "outputs": [],
      "source": [
        "middle_index = (x_train_raw.shape[1] // 2) + 1 # This is location for Modified Sequence . Use this as Y Target\n",
        "\n",
        "if CROP_STRATEGY == 'MID':\n",
        "    STRAT_INEDX =middle_index - WINDOW_SIZE -1\n",
        "    END_INDEX =middle_index + WINDOW_SIZE\n",
        "\n",
        "if CROP_STRATEGY == 'END':\n",
        "    STRAT_INEDX =middle_index - (WINDOW_SIZE*2) -1\n",
        "    END_INDEX =middle_index # Ignore Modified Position\n",
        "\n",
        "x_data_cropped =  x_data.iloc[:,STRAT_INEDX :END_INDEX]\n",
        "\n",
        "concatenated_column= x_data_cropped.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
        "x_data_cropped = x_data_cropped.assign(Sequence=concatenated_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wVvFs6j93Jr",
        "outputId": "7b7d0a73-0b54-4ad8-9dd7-f8e3c89d273a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X : (309460, 102) \n"
          ]
        }
      ],
      "source": [
        "print(f\" Shape of X : {x_data_cropped.shape} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKBG_T9M93Jt"
      },
      "source": [
        "### Encode with 3-mer Codon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rTx7-cVt93Jt"
      },
      "outputs": [],
      "source": [
        "kmer_dict = {}\n",
        "k = 3\n",
        "with open(ENCODING_FILE, 'rb') as f:\n",
        "    kmer_dict = pickle.load(f)\n",
        "\n",
        "\n",
        "def encode_with_k_mer_codon(sequence):\n",
        "    encoded_sequence = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        encoded_sequence.append(kmer_dict[sequence[i:i+k]] )\n",
        "\n",
        "    return np.array(encoded_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DimAoBPH93Ju"
      },
      "source": [
        "### Encode Target Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iNjMg64s93Ju"
      },
      "outputs": [],
      "source": [
        "RMs = ['A','G','C','T']\n",
        "RMEncoding = [0,1,2,3]\n",
        "\n",
        "All_RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
        "All_RMEncoding = [12,1,2,3,4,5,6,7,8,9,10,11,0]\n",
        "\n",
        "\n",
        "def convert_y_to_original_labels(row):\n",
        "    label = \"\"\n",
        "    for index , n in enumerate(row.tolist()) :\n",
        "        if n == 1 :\n",
        "            label = All_RMs[index]\n",
        "    if label == '':\n",
        "        return 'NonMoD'\n",
        "    return label\n",
        "\n",
        "def encode_target(y_data):\n",
        "    # Write Customer Lable Encoder . This is required since we have train and test alreday splitted. Always creating a new instanc of label encoder will change encoding.\n",
        "    y_encoded = []\n",
        "    for y in y_data:\n",
        "        index = RMs.index(y)\n",
        "        encoding =  RMEncoding[index]\n",
        "        y_encoded.append(encoding)\n",
        "    return y_encoded\n",
        "\n",
        "def prepare_data_for_binary_classification(x_data , y_data , prediction_class):\n",
        "    # Convert One Hot Encoded Y to to Original Labels\n",
        "    y_original_labels = y_data.apply(convert_y_to_original_labels,axis=1)\n",
        "    x_data['Label'] = y_original_labels\n",
        "    target_class = prediction_class\n",
        "    selected_rna_data = x_data[x_data['Label'].isin(target_class)]\n",
        "\n",
        "    y_filtered = selected_rna_data['Label']\n",
        "    x_filtered = selected_rna_data.drop('Label', axis=1)\n",
        "\n",
        "    return x_filtered , y_filtered\n",
        "\n",
        "\n",
        "modification_list= ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol']\n",
        "\n",
        "x_data_filtered , y_data_filtered = prepare_data_for_binary_classification(x_data_cropped , y_data , modification_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZfM1jN1S_UN",
        "outputId": "1db27947-b4f3-4487-9bd0-4d9dc58c4520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         A\n",
              "1         A\n",
              "2         A\n",
              "3         A\n",
              "4         A\n",
              "         ..\n",
              "309305    A\n",
              "309306    A\n",
              "309307    A\n",
              "309308    A\n",
              "309309    A\n",
              "Name: 500, Length: 154853, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x_data_filtered.iloc[:,x_data_filtered.shape[1]-52]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i_tmMT7g_d0o"
      },
      "outputs": [],
      "source": [
        "x_data = None\n",
        "y_data = None\n",
        "y_valid_raw = None\n",
        "x_valid_raw = None\n",
        "x_train_raw = None\n",
        "y_train_raw = None\n",
        "x_test_raw = None\n",
        "y_test_raw = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I41qOgy93Ju",
        "outputId": "67f61b8e-d549-47e8-9960-31b13478af3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode X\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e6286ae6e1e0>:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode Y\n",
            "A    138175\n",
            "T      9086\n",
            "C      5085\n",
            "G      2507\n",
            "Name: 500, dtype: int64\n",
            "Generate Train and Split..\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Encode X\")\n",
        "X_encoded  =  x_data_filtered['Sequence'].apply(encode_with_k_mer_codon)\n",
        "X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n",
        "\n",
        "\n",
        "print(\"Encode Y\")\n",
        "y_originals = x_data_filtered.iloc[:,x_data_filtered.shape[1]-52] # Using Crop as MID Strategy - Get Middle Position as Target Label\n",
        "print(y_originals.value_counts())\n",
        "y_encoded = encode_target(y_originals)\n",
        "\n",
        "\n",
        "print(\"Generate Train and Split..\")\n",
        "# Train set\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test and Validation set\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWYjil19IK70",
        "outputId": "167d92fc-0470-481e-e692-a2641ca961ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([108397, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT9mzuZN93Ju"
      },
      "source": [
        "### Destroy Object for Memory Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lHvM91W193Ju"
      },
      "outputs": [],
      "source": [
        "x_data_cropped = None\n",
        "y_encoded = None\n",
        "y_originals = None\n",
        "X_encoded = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rBSNkFLt1Bt"
      },
      "source": [
        "### Apply Sampling on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XzsQXNB493Jv"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(y_train , dtype=torch.long)\n",
        "y_test = torch.tensor(y_test , dtype=torch.long)\n",
        "y_valid = torch.tensor(y_valid , dtype=torch.long)\n",
        "\n",
        "sm = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
        "y_resampled = torch.tensor(y_resampled , dtype=torch.float32) # Keeping float32\n",
        "X_resampled = torch.tensor(X_resampled , dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoeMDV2PzOqF",
        "outputId": "a6608feb-fbde-4a2a-b6e7-4db4372901a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[50., 50., 43.,  ...,  2., 33., 52.],\n",
              "        [11., 60., 48.,  ..., 53., 59., 37.],\n",
              "        [ 2., 61., 50.,  ..., 45., 51., 39.],\n",
              "        ...,\n",
              "        [53., 59., 37.,  ..., 48., 56., 55.],\n",
              "        [50., 50., 50.,  ..., 26., 34., 35.],\n",
              "        [ 1., 19., 56.,  ..., 56., 55., 61.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "X_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhyfGHeoS_UP",
        "outputId": "0e126dbc-d852-468f-c127-d0aec6c3d817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value: 0.0, Count: 96680\n",
            "Value: 1.0, Count: 96680\n",
            "Value: 2.0, Count: 96680\n",
            "Value: 3.0, Count: 96680\n"
          ]
        }
      ],
      "source": [
        "y_resampled_np = y_resampled.numpy()\n",
        "# Get unique values and their counts\n",
        "unique_values, counts = np.unique(y_resampled_np, return_counts=True)\n",
        "# Print unique values and their counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5mvgY8jp93Jv"
      },
      "outputs": [],
      "source": [
        "hyperparameter = {}\n",
        "hyperparameter['INPUT_DIMENSION'] = len(kmer_dict) # For One Hot Encoding Input Dimension would be 4 as there only 4 unique nucleocide\n",
        "hyperparameter['HIDDEN_DIMENSION'] = 128\n",
        "hyperparameter['NO_OF_LAYERS'] = 1\n",
        "hyperparameter['BATCH_SIZE'] = 32\n",
        "hyperparameter['OUTPUT_DIMENSION'] = 4\n",
        "hyperparameter['EMBEDDING_DIMENSION'] = 7 # if you are using Word2Vec Encoding then this should be same as Word2Vec Embedding Dim\n",
        "hyperparameter['DROP_OUT'] = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BmZPlsyH93Jv"
      },
      "outputs": [],
      "source": [
        "class RNADataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "# Always use ReSampling for Training\n",
        "train_dataset = RNADataset(X_resampled, y_resampled)\n",
        "test_dataset = RNADataset(X_test, y_test)\n",
        "valid_dataset = RNADataset(X_valid, y_valid)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBgi-ML3tnFy"
      },
      "source": [
        "### RNA with Attention Mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Euih9dUZC33l"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden.unsqueeze(1)\n",
        "        hidden = hidden.expand(-1, encoder_outputs.size(1), -1)\n",
        "        alignment_scores = self.v(torch.tanh(self.W(hidden + encoder_outputs)))\n",
        "        attention_weights = F.softmax(alignment_scores, dim=1)\n",
        "        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class RNABasicNucleosidePredictionModel(nn.Module):\n",
        "    def __init__(self, input_dim,embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n",
        "        super(RNABasicNucleosidePredictionModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.attention = BahdanauAttention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        x_embeded = self.embedding(x)\n",
        "\n",
        "        lstm_out, (h, c) = self.lstm(x_embeded)\n",
        "\n",
        "        hidden = h.squeeze(0)\n",
        "\n",
        "        #apply attention using the last hidden state and the encoder outputs\n",
        "        context_vector, attention_weights = self.attention(hidden, lstm_out)\n",
        "\n",
        "        #dropout\n",
        "        output = self.dropout(context_vector)\n",
        "\n",
        "        #final output\n",
        "        out = self.fc(output)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ygsOZTQE93Jv"
      },
      "outputs": [],
      "source": [
        "# class RNABasicNucleosidePredictionModel(nn.Module):\n",
        "#     def __init__(self, input_dim,embedding_dim, hidden_dim,num_layers, output_dim , dropout=0.5):\n",
        "#         super(RNABasicNucleosidePredictionModel, self).__init__()\n",
        "\n",
        "#         # Pytorch Embedding\n",
        "#         self.embedding = nn.Embedding(input_dim,embedding_dim)\n",
        "\n",
        "#         # Bi-Directional LSTM Model\n",
        "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout= dropout, batch_first=True )\n",
        "\n",
        "#         # Fully Connected Layer\n",
        "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "#         # Drop out layer for Overfitting\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x  = x.long()\n",
        "#         x_embeded =  self.embedding(x)\n",
        "\n",
        "#         lstm_out, (h,c) = self.lstm(x_embeded)\n",
        "\n",
        "#         # Pass it to drop out layer\n",
        "#         output = self.dropout(h[-1])\n",
        "\n",
        "#         # Finally pass it to fully connected layer.\n",
        "#         out = self.fc(output)\n",
        "\n",
        "#         return out\n",
        "\n",
        "\n",
        "def validate_model(model, test_dataloader , device ,loss_function):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    class_correct = [0] * hyperparameter['OUTPUT_DIMENSION']\n",
        "    class_total = [0] * hyperparameter['OUTPUT_DIMENSION']\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())  # Capture True Lables for Summary Report\n",
        "            predicted_labels.extend(predicted.cpu().numpy()) # Capture Predicted Labels Lables for Summary Report\n",
        "\n",
        "    validation_loss = running_loss / len(test_dataloader)\n",
        "    validation_accuracy = correct / total\n",
        "\n",
        "    return validation_loss , validation_accuracy , true_labels , predicted_labels\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, device, epochs, optimizer, loss_function):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        val_loss,  validation_accuracy , true_labels , predicted_labels = validate_model(model, test_dataloader, device, loss_function)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Test Accuracy: {validation_accuracy:.4f} , Time Taken : {elapsed_time}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count == 10:\n",
        "                print(\"No improvement in validation loss for 5 epochs. Training stopped.\")\n",
        "                break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4o06niq93Jw",
        "outputId": "e8c2d969-6ac3-422a-ff68-8ee2f68bfb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Parameters for Model Training : 87959 \n",
            "Model Parameters  :  {'INPUT_DIMENSION': 94, 'HIDDEN_DIMENSION': 128, 'NO_OF_LAYERS': 1, 'BATCH_SIZE': 32, 'OUTPUT_DIMENSION': 4, 'EMBEDDING_DIMENSION': 7, 'DROP_OUT': 0.1}\n",
            "Epoch 1, Train Loss: 1.3099, Val Loss: 1.2770, Test Accuracy: 0.4526 , Time Taken : 45.182049036026\n",
            "Epoch 2, Train Loss: 1.2819, Val Loss: 1.3413, Test Accuracy: 0.3318 , Time Taken : 45.96895098686218\n",
            "Epoch 3, Train Loss: 0.9261, Val Loss: 0.4760, Test Accuracy: 0.8131 , Time Taken : 45.23568272590637\n",
            "Epoch 4, Train Loss: 0.3026, Val Loss: 0.1128, Test Accuracy: 0.9718 , Time Taken : 45.2831084728241\n",
            "Epoch 5, Train Loss: 0.1121, Val Loss: 0.0383, Test Accuracy: 0.9902 , Time Taken : 46.17015075683594\n",
            "Epoch 6, Train Loss: 0.1275, Val Loss: 0.0281, Test Accuracy: 0.9935 , Time Taken : 45.27784252166748\n",
            "Epoch 7, Train Loss: 0.0411, Val Loss: 0.0238, Test Accuracy: 0.9943 , Time Taken : 45.442524433135986\n",
            "Epoch 8, Train Loss: 0.2531, Val Loss: 0.0261, Test Accuracy: 0.9944 , Time Taken : 46.11104989051819\n",
            "Epoch 9, Train Loss: 0.1659, Val Loss: 0.0059, Test Accuracy: 0.9997 , Time Taken : 45.35626554489136\n",
            "Epoch 10, Train Loss: 0.0262, Val Loss: 0.0020, Test Accuracy: 1.0000 , Time Taken : 45.465532064437866\n",
            "Epoch 11, Train Loss: 0.0101, Val Loss: 0.0005, Test Accuracy: 1.0000 , Time Taken : 46.28535079956055\n",
            "Epoch 12, Train Loss: 0.0164, Val Loss: 0.0003, Test Accuracy: 1.0000 , Time Taken : 47.211246967315674\n",
            "Epoch 13, Train Loss: 0.1005, Val Loss: 0.0086, Test Accuracy: 0.9978 , Time Taken : 45.984763383865356\n",
            "Epoch 14, Train Loss: 1.3227, Val Loss: 1.2665, Test Accuracy: 0.4380 , Time Taken : 45.585304260253906\n",
            "Epoch 15, Train Loss: 0.6448, Val Loss: 0.0430, Test Accuracy: 0.9845 , Time Taken : 45.13701033592224\n",
            "Epoch 16, Train Loss: 0.7305, Val Loss: 0.7257, Test Accuracy: 0.7106 , Time Taken : 45.44163775444031\n",
            "Epoch 17, Train Loss: 0.3047, Val Loss: 0.0147, Test Accuracy: 0.9991 , Time Taken : 45.76784110069275\n",
            "Epoch 18, Train Loss: 0.2802, Val Loss: 0.0069, Test Accuracy: 0.9993 , Time Taken : 45.124433517456055\n",
            "Epoch 19, Train Loss: 0.3672, Val Loss: 0.0021, Test Accuracy: 1.0000 , Time Taken : 45.529218673706055\n",
            "Epoch 20, Train Loss: 0.1051, Val Loss: 0.0001, Test Accuracy: 1.0000 , Time Taken : 46.07551908493042\n"
          ]
        }
      ],
      "source": [
        "model = RNABasicNucleosidePredictionModel(input_dim=hyperparameter['INPUT_DIMENSION'],\n",
        "                            embedding_dim=hyperparameter['EMBEDDING_DIMENSION'],\n",
        "                            hidden_dim=hyperparameter['HIDDEN_DIMENSION'] ,\n",
        "                            num_layers = hyperparameter['NO_OF_LAYERS'],\n",
        "                            output_dim=hyperparameter['OUTPUT_DIMENSION'],\n",
        "                            dropout=hyperparameter['DROP_OUT'] )\n",
        "\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()  ## MSELoss of Regression problem  # BCELoss for binary classification\n",
        "optimizer = optim.Adam(model.parameters() ,  lr=0.0001)\n",
        "\n",
        "# Number of Parameters for Model\n",
        "total_parameters = []\n",
        "for p in model.parameters():\n",
        "    total_parameters.append(p.numel())\n",
        "\n",
        "print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model Parameters  : \" , hyperparameter)\n",
        "\n",
        "# Train Model with configured Parameter\n",
        "train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey7D9iLG93Jw",
        "outputId": "4c68b1b8-ecb4-4aed-8249-67d6c1dcdfe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy: 1.0000\n",
            "\n",
            " Classification Summary:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     20751\n",
            "           1       1.00      1.00      1.00       359\n",
            "           2       1.00      1.00      1.00       767\n",
            "           3       1.00      1.00      1.00      1351\n",
            "\n",
            "    accuracy                           1.00     23228\n",
            "   macro avg       1.00      1.00      1.00     23228\n",
            "weighted avg       1.00      1.00      1.00     23228\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "_, final_accuracy, true_labels, predicted_labels = validate_model(model, valid_dataloader,device,loss_function)\n",
        "\n",
        "# Print the final accuracy\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Print the classification summary\n",
        "print(\"\\n Classification Summary:\")\n",
        "print(classification_report(true_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path =  '/content/drive/My Drive/Colab Notebooks/Capstone Project/model/4_nucleoside_model.pt'\n",
        "torch.save(model, model_path)"
      ],
      "metadata": {
        "id": "wwQezisdV8P6"
      },
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6L74k59u93Js"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}