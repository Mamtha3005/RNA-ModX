{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OhuWQTdS93Jn"
      },
      "source": [
        "# RNA 4 Nucleoside Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZ_i_0-j93Jp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        " #Import All Libraries Here\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score ,  roc_curve, auc , classification_report\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import time\n",
        "from collections import Counter\n",
        "# PyTorch Import\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fDtq9jF-E8l",
        "outputId": "af3b931a-e94b-4cd0-e2b4-dd7a6abee9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.5)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.1)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna  scikit-learn gensim imbalanced-learn xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6FRymYz97o7",
        "outputId": "8727a455-1ea2-461b-d78f-a61355518bc3"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# INPUT_TRAIN_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/train_in.csv\"\n",
        "# INPUT_TRAIN_OUT = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/train_out.csv\"\n",
        "# INPUT_TEST_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/test_in.csv\"\n",
        "# INPUT_TEST_OUT = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/test_out.csv\"\n",
        "# INPUT_VALIDATION_IN = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/valid_in_nucleo.csv\"\n",
        "# INPUT_VALIDATION_OUT  = \"/content/drive/My Drive/Colab Notebooks/Capstone Project/data/valid_out.csv\"\n",
        "\n",
        "# Record Constants\n",
        "INPUT_TRAIN_IN = \"../../../data/train_in.csv\"\n",
        "INPUT_TRAIN_OUT = \"../../../data/train_out.csv\"\n",
        "INPUT_TEST_IN = \"../../../data/test_in.csv\"\n",
        "INPUT_TEST_OUT = \"../../../data/test_out.csv\"\n",
        "INPUT_VALIDATION_IN = \"../../../data/valid_in_nucleo.csv\"\n",
        "INPUT_VALIDATION_OUT  = \"../../../data/valid_out.csv\"\n",
        "\n",
        "TARGET_MODEL_PATH = '../../webapp/model_files'\n",
        "ENCODING_FILE = '3-mer-dictionary.pkl'\n",
        "\n",
        "WINDOW_SIZE = 50\n",
        "\n",
        "# Startegy to Crop Sequene\n",
        "# MID - Modification is present at Mid of cropped Sequence\n",
        "# END - Modification is present at End of cropepd Sequence\n",
        "CROP_STRATEGY = 'MID'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7HMGVZeh93Jq"
      },
      "outputs": [],
      "source": [
        "#Read X Varaibles and Y Varaibles\n",
        "\n",
        "x_train_raw =  pd.read_csv(INPUT_TRAIN_IN, header=None , skiprows=1  )\n",
        "y_train_raw =  pd.read_csv(INPUT_TRAIN_OUT, header=None , skiprows=1 )\n",
        "\n",
        "x_test_raw =  pd.read_csv(INPUT_TEST_IN, header=None , skiprows=1 )\n",
        "y_test_raw =  pd.read_csv(INPUT_TEST_OUT, header=None , skiprows=1)\n",
        "\n",
        "x_valid_raw =  pd.read_csv(INPUT_VALIDATION_IN, header=None , skiprows=1 )\n",
        "y_valid_raw =  pd.read_csv(INPUT_VALIDATION_OUT, header=None , skiprows=1 )\n",
        "\n",
        "\n",
        "x_data = pd.concat([x_train_raw, x_test_raw, x_valid_raw], axis=0, ignore_index=True)\n",
        "y_data = pd.concat([y_train_raw, y_test_raw, y_valid_raw], axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CkcDtmho93Jr"
      },
      "outputs": [],
      "source": [
        "middle_index = (x_train_raw.shape[1] // 2) + 1 # This is location for Modified Sequence . Use this as Y Target\n",
        "\n",
        "if CROP_STRATEGY == 'MID':\n",
        "    STRAT_INEDX =middle_index - WINDOW_SIZE -1\n",
        "    END_INDEX =middle_index + WINDOW_SIZE\n",
        "\n",
        "if CROP_STRATEGY == 'END':\n",
        "    STRAT_INEDX =middle_index - (WINDOW_SIZE*2) -1\n",
        "    END_INDEX =middle_index # Ignore Modified Position\n",
        "\n",
        "x_data_cropped =  x_data.iloc[:,STRAT_INEDX :END_INDEX]\n",
        "\n",
        "concatenated_column= x_data_cropped.apply(lambda row: ''.join(map(str, row)), axis=1)\n",
        "x_data_cropped = x_data_cropped.assign(Sequence=concatenated_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wVvFs6j93Jr",
        "outputId": "95eb9bf9-6b1c-43b9-812f-473f14768573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Shape of X : (309460, 102) \n"
          ]
        }
      ],
      "source": [
        "print(f\" Shape of X : {x_data_cropped.shape} \")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bKBG_T9M93Jt"
      },
      "source": [
        "### Encode with 3-mer Codon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rTx7-cVt93Jt"
      },
      "outputs": [],
      "source": [
        "kmer_dict = {}\n",
        "k = 3\n",
        "with open(ENCODING_FILE, 'rb') as f:\n",
        "    kmer_dict = pickle.load(f)\n",
        "\n",
        "\n",
        "def encode_with_k_mer_codon(sequence):\n",
        "    encoded_sequence = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        encoded_sequence.append(kmer_dict[sequence[i:i+k]] )\n",
        "\n",
        "    return np.array(encoded_sequence)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DimAoBPH93Ju"
      },
      "source": [
        "### Encode Target Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iNjMg64s93Ju"
      },
      "outputs": [],
      "source": [
        "RMs = ['A','G','C','T']\n",
        "RMEncoding = [0,1,2,3]\n",
        "\n",
        "All_RMs = ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol','NonMoD']\n",
        "All_RMEncoding = [12,1,2,3,4,5,6,7,8,9,10,11,0]\n",
        "\n",
        "\n",
        "def convert_y_to_original_labels(row):\n",
        "    label = \"\"\n",
        "    for index , n in enumerate(row.tolist()) :\n",
        "        if n == 1 :\n",
        "            label = All_RMs[index]\n",
        "    if label == '':\n",
        "        return 'NonMoD'\n",
        "    return label\n",
        "\n",
        "def encode_target(y_data):\n",
        "    # Write Customer Lable Encoder . This is required since we have train and test alreday splitted. Always creating a new instanc of label encoder will change encoding.\n",
        "    y_encoded = []\n",
        "    for y in y_data:\n",
        "        index = RMs.index(y)\n",
        "        encoding =  RMEncoding[index]\n",
        "        y_encoded.append(encoding)\n",
        "    return y_encoded\n",
        "\n",
        "def prepare_data_for_binary_classification(x_data , y_data , prediction_class):\n",
        "    # Convert One Hot Encoded Y to to Original Labels\n",
        "    y_original_labels = y_data.apply(convert_y_to_original_labels,axis=1)\n",
        "    x_data['Label'] = y_original_labels\n",
        "    target_class = prediction_class\n",
        "    selected_rna_data = x_data[x_data['Label'].isin(target_class)]\n",
        "\n",
        "    y_filtered = selected_rna_data['Label']\n",
        "    x_filtered = selected_rna_data.drop('Label', axis=1)\n",
        "\n",
        "    return x_filtered , y_filtered\n",
        "\n",
        "\n",
        "modification_list= ['hAm','hCm','hGm','hTm','hm1A','hm5C','hm5U','hm6A','hm6Am','hm7G','hPsi','Atol']\n",
        "\n",
        "x_data_filtered , y_data_filtered = prepare_data_for_binary_classification(x_data_cropped , y_data , modification_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         A\n",
              "1         A\n",
              "2         A\n",
              "3         A\n",
              "4         A\n",
              "         ..\n",
              "309305    A\n",
              "309306    A\n",
              "309307    A\n",
              "309308    A\n",
              "309309    A\n",
              "Name: 500, Length: 154853, dtype: object"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_data_filtered.iloc[:,x_data_filtered.shape[1]-52]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "i_tmMT7g_d0o"
      },
      "outputs": [],
      "source": [
        "x_data = None\n",
        "y_data = None\n",
        "y_valid_raw = None\n",
        "x_valid_raw = None\n",
        "x_train_raw = None\n",
        "y_train_raw = None\n",
        "x_test_raw = None\n",
        "y_test_raw = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I41qOgy93Ju",
        "outputId": "faec99f7-04d6-4453-af26-4ffbd2d07526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encode X\n",
            "Encode Y\n",
            "A    138175\n",
            "T      9086\n",
            "C      5085\n",
            "G      2507\n",
            "Name: 500, dtype: int64\n",
            "Generate Train and Split..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shashi.vish\\AppData\\Local\\Temp\\ipykernel_10128\\1009323598.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
            "  X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Encode X\")\n",
        "X_encoded  =  x_data_filtered['Sequence'].apply(encode_with_k_mer_codon)\n",
        "X_encoded = torch.tensor(X_encoded , dtype =  torch.long)\n",
        "\n",
        "\n",
        "print(\"Encode Y\")\n",
        "y_originals = x_data_filtered.iloc[:,x_data_filtered.shape[1]-52] # Using Crop as MID Strategy - Get Middle Position as Target Label\n",
        "print(y_originals.value_counts())\n",
        "y_encoded = encode_target(y_originals)\n",
        "\n",
        "\n",
        "print(\"Generate Train and Split..\")\n",
        "# Train set\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test and Validation set\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWYjil19IK70",
        "outputId": "365d5d2f-87d5-4ab7-8fc8-130852baecd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([108397, 99])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zT9mzuZN93Ju"
      },
      "source": [
        "### Destroy Object for Memory Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lHvM91W193Ju"
      },
      "outputs": [],
      "source": [
        "x_data_cropped = None\n",
        "y_encoded = None\n",
        "y_originals = None\n",
        "X_encoded = None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-rBSNkFLt1Bt"
      },
      "source": [
        "### Apply Sampling on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzsQXNB493Jv",
        "outputId": "94fbaf3f-4e64-4599-f2ed-454603334670"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(y_train , dtype=torch.long)\n",
        "y_test = torch.tensor(y_test , dtype=torch.long)\n",
        "y_valid = torch.tensor(y_valid , dtype=torch.long)\n",
        "\n",
        "sm = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
        "y_resampled = torch.tensor(y_resampled , dtype=torch.float32) # Keeping float32\n",
        "X_resampled = torch.tensor(X_resampled , dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoeMDV2PzOqF",
        "outputId": "5a741437-56f8-4e84-b65e-7b16899878f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[50., 50., 43.,  ...,  2., 33., 52.],\n",
              "        [11., 60., 48.,  ..., 53., 59., 37.],\n",
              "        [ 2., 61., 50.,  ..., 45., 51., 39.],\n",
              "        ...,\n",
              "        [53., 59., 37.,  ..., 48., 56., 55.],\n",
              "        [50., 50., 50.,  ..., 26., 34., 35.],\n",
              "        [ 1., 19., 56.,  ..., 56., 55., 61.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_resampled_np = y_resampled.numpy()\n",
        "# Get unique values and their counts\n",
        "unique_values, counts = np.unique(y_resampled_np, return_counts=True)\n",
        "# Print unique values and their counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5mvgY8jp93Jv"
      },
      "outputs": [],
      "source": [
        "hyperparameter = {}\n",
        "hyperparameter['INPUT_DIMENSION'] = len(kmer_dict) # For One Hot Encoding Input Dimension would be 4 as there only 4 unique nucleocide\n",
        "hyperparameter['HIDDEN_DIMENSION'] = 32\n",
        "hyperparameter['NO_OF_LAYERS'] = 1\n",
        "hyperparameter['BATCH_SIZE'] = 32\n",
        "hyperparameter['OUTPUT_DIMENSION'] = 4\n",
        "hyperparameter['EMBEDDING_DIMENSION'] = 7 # if you are using Word2Vec Encoding then this should be same as Word2Vec Embedding Dim\n",
        "hyperparameter['DROP_OUT'] = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BmZPlsyH93Jv"
      },
      "outputs": [],
      "source": [
        "class RNADataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "# Always use ReSampling for Training\n",
        "train_dataset = RNADataset(X_resampled, y_resampled)\n",
        "test_dataset = RNADataset(X_test, y_test)\n",
        "valid_dataset = RNADataset(X_valid, y_valid)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=hyperparameter['BATCH_SIZE'], shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LBgi-ML3tnFy"
      },
      "source": [
        "### RNA with Attention Mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Euih9dUZC33l"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden.unsqueeze(1)\n",
        "        hidden = hidden.expand(-1, encoder_outputs.size(1), -1)\n",
        "        alignment_scores = self.v(torch.tanh(self.W(hidden + encoder_outputs)))\n",
        "        attention_weights = F.softmax(alignment_scores, dim=1)\n",
        "        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class RNABasicNucleosidePredictionModel(nn.Module):\n",
        "    def __init__(self, input_dim,embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n",
        "        super(RNABasicNucleosidePredictionModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.attention = BahdanauAttention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        x_embeded = self.embedding(x)\n",
        "\n",
        "        lstm_out, (h, c) = self.lstm(x_embeded)\n",
        "\n",
        "        hidden = h.squeeze(0)\n",
        "\n",
        "        #apply attention using the last hidden state and the encoder outputs\n",
        "        context_vector, attention_weights = self.attention(hidden, lstm_out)\n",
        "\n",
        "        #dropout\n",
        "        output = self.dropout(context_vector)\n",
        "\n",
        "        #final output\n",
        "        out = self.fc(output)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ygsOZTQE93Jv"
      },
      "outputs": [],
      "source": [
        "# class RNABasicNucleosidePredictionModel(nn.Module):\n",
        "#     def __init__(self, input_dim,embedding_dim, hidden_dim,num_layers, output_dim , dropout=0.5):\n",
        "#         super(RNABasicNucleosidePredictionModel, self).__init__()\n",
        "\n",
        "#         # Pytorch Embedding\n",
        "#         self.embedding = nn.Embedding(input_dim,embedding_dim)\n",
        "\n",
        "#         # Bi-Directional LSTM Model\n",
        "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout= dropout, batch_first=True )\n",
        "\n",
        "#         # Fully Connected Layer\n",
        "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "#         # Drop out layer for Overfitting\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x  = x.long()\n",
        "#         x_embeded =  self.embedding(x)\n",
        "\n",
        "#         lstm_out, (h,c) = self.lstm(x_embeded)\n",
        "\n",
        "#         # Pass it to drop out layer\n",
        "#         output = self.dropout(h[-1])\n",
        "\n",
        "#         # Finally pass it to fully connected layer.\n",
        "#         out = self.fc(output)\n",
        "\n",
        "#         return out\n",
        "\n",
        "\n",
        "def validate_model(model, test_dataloader , device ,loss_function):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    class_correct = [0] * hyperparameter['OUTPUT_DIMENSION']\n",
        "    class_total = [0] * hyperparameter['OUTPUT_DIMENSION']\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())  # Capture True Lables for Summary Report\n",
        "            predicted_labels.extend(predicted.cpu().numpy()) # Capture Predicted Labels Lables for Summary Report\n",
        "\n",
        "    validation_loss = running_loss / len(test_dataloader)\n",
        "    validation_accuracy = correct / total\n",
        "\n",
        "    return validation_loss , validation_accuracy , true_labels , predicted_labels\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, device, epochs, optimizer, loss_function):\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        val_loss,  validation_accuracy , true_labels , predicted_labels = validate_model(model, test_dataloader, device, loss_function)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Test Accuracy: {validation_accuracy:.4f} , Time Taken : {elapsed_time}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count == 10:\n",
        "                print(\"No improvement in validation loss for 5 epochs. Training stopped.\")\n",
        "                break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4o06niq93Jw",
        "outputId": "af413665-e5f4-434c-a4b1-badf412f4fa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shashi.vish\\Python Environment\\RNA_ModX\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Number of Parameters for Model Training : 7127 \n",
            "Model Parameters  :  {'INPUT_DIMENSION': 94, 'HIDDEN_DIMENSION': 32, 'NO_OF_LAYERS': 1, 'BATCH_SIZE': 32, 'OUTPUT_DIMENSION': 4, 'EMBEDDING_DIMENSION': 7, 'DROP_OUT': 0.1}\n",
            "Epoch 1, Train Loss: 1.3225, Val Loss: 1.2844, Test Accuracy: 0.4612 , Time Taken : 178.15017867088318\n",
            "Epoch 2, Train Loss: 1.2971, Val Loss: 1.2999, Test Accuracy: 0.4387 , Time Taken : 155.09025263786316\n",
            "Epoch 3, Train Loss: 1.2817, Val Loss: 1.2486, Test Accuracy: 0.4616 , Time Taken : 272.69124460220337\n",
            "Epoch 4, Train Loss: 1.2585, Val Loss: 1.2117, Test Accuracy: 0.5126 , Time Taken : 286.7763066291809\n",
            "Epoch 5, Train Loss: 1.1763, Val Loss: 1.0406, Test Accuracy: 0.5966 , Time Taken : 291.8746795654297\n",
            "Epoch 6, Train Loss: 1.0042, Val Loss: 0.8548, Test Accuracy: 0.6528 , Time Taken : 318.1101083755493\n",
            "Epoch 7, Train Loss: 0.8280, Val Loss: 0.7378, Test Accuracy: 0.6787 , Time Taken : 291.75079464912415\n",
            "Epoch 8, Train Loss: 0.6599, Val Loss: 0.6102, Test Accuracy: 0.7185 , Time Taken : 291.566956281662\n",
            "Epoch 9, Train Loss: 0.4654, Val Loss: 0.4378, Test Accuracy: 0.8076 , Time Taken : 134.1436731815338\n",
            "Epoch 10, Train Loss: 0.2983, Val Loss: 0.2628, Test Accuracy: 0.8739 , Time Taken : 88.57871580123901\n"
          ]
        }
      ],
      "source": [
        "model = RNABasicNucleosidePredictionModel(input_dim=hyperparameter['INPUT_DIMENSION'],\n",
        "                            embedding_dim=hyperparameter['EMBEDDING_DIMENSION'],\n",
        "                            hidden_dim=hyperparameter['HIDDEN_DIMENSION'] ,\n",
        "                            num_layers = hyperparameter['NO_OF_LAYERS'],\n",
        "                            output_dim=hyperparameter['OUTPUT_DIMENSION'],\n",
        "                            dropout=hyperparameter['DROP_OUT'] )\n",
        "\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()  ## MSELoss of Regression problem  # BCELoss for binary classification\n",
        "optimizer = optim.Adam(model.parameters() ,  lr=0.0001)\n",
        "\n",
        "# Number of Parameters for Model\n",
        "total_parameters = []\n",
        "for p in model.parameters():\n",
        "    total_parameters.append(p.numel())\n",
        "\n",
        "print(f\"Total Number of Parameters for Model Training : { sum(total_parameters)} \" )\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model Parameters  : \" , hyperparameter)\n",
        "\n",
        "# Train Model with configured Parameter\n",
        "train_model(model, train_dataloader ,test_dataloader, device ,num_epochs,optimizer,loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey7D9iLG93Jw",
        "outputId": "f883372e-4a21-490a-9645-fd1d667583a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Accuracy: 0.8743\n",
            "\n",
            " Classification Summary:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.87      0.93     20751\n",
            "           1       0.74      0.95      0.83       359\n",
            "           2       0.28      0.94      0.44       767\n",
            "           3       0.58      0.95      0.72      1351\n",
            "\n",
            "    accuracy                           0.87     23228\n",
            "   macro avg       0.65      0.93      0.73     23228\n",
            "weighted avg       0.95      0.87      0.90     23228\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "_, final_accuracy, true_labels, predicted_labels = validate_model(model, valid_dataloader,device,loss_function)\n",
        "\n",
        "# Print the final accuracy\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Print the classification summary\n",
        "print(\"\\n Classification Summary:\")\n",
        "print(classification_report(true_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A\n"
          ]
        }
      ],
      "source": [
        "rna_sequence = \"TTGCCACACTGCTGGACGCCTGCAAGGCCAAGGGTACGGAGGTCATCATCATCACCACCGATACCTCGCCCTCAGGCACCAAGAAGACCCGGCAGTATCTC\"\n",
        "print(rna_sequence[50])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6L74k59u93Js"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
