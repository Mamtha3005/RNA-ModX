{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_sequnce = ['ACT','GTU' , 'AUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_seq(kmer_token):\n",
    "\n",
    "    # A 1 0 0 0\n",
    "    # C 0 1 0 0\n",
    "    # T/U 0 0 0 1\n",
    "    # G 0 0 1 0\n",
    "    # N 0 0 0 0\n",
    "\n",
    "    encoding_dict = {\n",
    "        'A': [1, 0, 0, 0],\n",
    "        'C': [0, 1, 0, 0],\n",
    "        'G': [0, 0, 1, 0],\n",
    "        'T': [0, 0, 0, 1],\n",
    "        'U': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0],\n",
    "    }\n",
    "\n",
    "    encoded_sequence = []\n",
    "    for  base in kmer_token:\n",
    "        encoded_sequence.append(encoding_dict[base])\n",
    "    return np.array(encoded_sequence).flatten()\n",
    "\n",
    "x = []\n",
    "for seq in rna_sequnce:\n",
    "    x.append(encode_seq(seq))\n",
    "x = torch.from_numpy( np.array(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['red','blue','green']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "y = torch.from_numpy(np.array(y))\n",
    "y "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 3\n",
    "embedding_dim = 7\n",
    "input_dim = 12\n",
    "\n",
    "embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
    "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True )\n",
    "fc = nn.Linear(hidden_dim, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12, 7])\n"
     ]
    }
   ],
   "source": [
    "embedding_out =  embedding(x)\n",
    "print(embedding_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1908, -0.2606, -0.0924],\n",
       "        [ 0.1917, -0.2623, -0.0830],\n",
       "        [ 0.2672, -0.2596, -0.2928]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out , (h,c) = lstm(embedding_out)\n",
    "\n",
    "h[-1] # In case of Bi Directional use both output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0084,  0.2081, -0.5230],\n",
       "        [ 0.0107,  0.2096, -0.5206],\n",
       "        [-0.0017,  0.2090, -0.5449]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = fc(h[-1])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs,1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1403356790542603"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss = loss_function(outputs, y)\n",
    "loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 3\n",
    "embedding_dim = 7\n",
    "input_dim = 12\n",
    "\n",
    "b_embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
    "b_lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True , bidirectional=True )\n",
    "b_fc = nn.Linear(2*hidden_dim, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12, 7])\n"
     ]
    }
   ],
   "source": [
    "b_embedding_out =  embedding(x)\n",
    "print(b_embedding_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3274,  0.3477,  0.0392,  0.2940, -0.1465, -0.0589],\n",
       "        [-0.2995,  0.3459,  0.0437,  0.1803, -0.1686,  0.0204],\n",
       "        [-0.4515,  0.2758, -0.1362,  0.2935, -0.1449, -0.0585]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_lstm_out , (h,c) = lstm(embedding_out)\n",
    "\n",
    "#       [[[ 0.1242,  0.0155,  0.1971],\n",
    "#          [ 0.1305,  0.0160,  0.2074],\n",
    "#          [ 0.1432, -0.0180, -0.1529]],\n",
    "\n",
    "#         [[ 0.0721, -0.3585, -0.2495],\n",
    "#          [-0.0041, -0.4517, -0.1265],\n",
    "#          [ 0.0710, -0.3557, -0.2356]]]\n",
    "\n",
    "b_h = torch.cat((h[-2, :, :], h[-1, :, :]), dim=1)\n",
    "b_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        hidden = hidden.expand(-1, encoder_outputs.size(1), -1)\n",
    "        # Aboe two steps makes hidden dim similar to encoder_outputs\n",
    "        \n",
    "        alignment_scores = self.v(torch.tanh(self.W(hidden + encoder_outputs)))\n",
    "        attention_weights = F.softmax(alignment_scores, dim=1)\n",
    "        context_vector = torch.sum(attention_weights * encoder_outputs, dim=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3244,  0.2820, -0.0638,  0.2010, -0.1485,  0.0028],\n",
       "        [-0.3227,  0.2817, -0.0600,  0.2047, -0.1535, -0.0042],\n",
       "        [-0.3397,  0.2763, -0.0792,  0.1829, -0.1382,  0.0202]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = BahdanauAttention(2*hidden_dim)\n",
    "context_vector , weights = attention(b_h , b_lstm_out)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3206, -0.0334,  0.1136],\n",
       "        [ 0.3347, -0.0262,  0.1085],\n",
       "        [ 0.3144, -0.0332,  0.1178]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = fc(context_vector)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs,1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.106108546257019"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss = loss_function(outputs, y)\n",
    "loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA_ModX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
